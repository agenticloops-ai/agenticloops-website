[{"title":"Codebase Navigator","description":"An augmented LLM agent that explores codebases using retrieval, tools, and memory","slug":"agentic-ai-engineering/01-foundations/06-codebase-navigator","icon":"layers","body":"# Codebase Navigator\n\nAn agent that helps engineers explore and understand unfamiliar codebases. Point it at any GitHub repo, and it will clone, index, and answer questions using semantic search while maintaining memory across sessions.\n\n> Implements the [Augmented LLM](https://www.anthropic.com/engineering/building-effective-agents) pattern from Anthropic's \"Building Effective Agents\" ‚Äî an LLM enhanced with retrieval, tools, and memory.\n\n\n> **üìö Setup & Running:** See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for prerequisites, setup instructions, and how to run tutorials.\n\n## üéØ What You'll Learn\n\n- Understand the **Augmented LLM** as the foundation of all agentic patterns\n- Implement **retrieval augmentation (RAG)** with ChromaDB and sentence-transformers\n- Connect LLMs to **tools** they can invoke autonomously via the agentic loop\n- Add persistent **memory** to maintain context across sessions\n- Build a practical agent that explores real codebases\n\n## üì¶ Available Examples\n\n| # | Script | Provider | What it demonstrates |\n|---|--------|----------|---------------------|\n| 01 | `01_codebase_navigator.py` | ![Anthropic](https://img.shields.io/badge/Anthropic-191919?style=for-the-badge&logo=anthropic&logoColor=white) | Full augmented LLM with RAG, tools, and memory |\n\n> **Contributions welcome!** We're looking for help porting this tutorial to additional providers. See [#13 ‚Äî Port to OpenAI API](https://github.com/agenticloops-ai/ai-agents-engineering/issues/13) if you'd like to contribute.\n\n## üîë Key Concepts\n\n### Augmentations\n\n**Retrieval (RAG)** ‚Äî Semantic search over indexed codebases using ChromaDB and sentence-transformers. The agent generates search queries to find relevant code chunks based on meaning, not just keywords.\n\n| Component | Description |\n|-----------|-------------|\n| **Vector Store** | Local ChromaDB containing embedded code chunks |\n| **Chunking** | Tree-sitter for AST-aware chunking (functions, classes, modules) |\n| **Embeddings** | Sentence-transformers (`all-MiniLM-L6-v2`) for local embeddings |\n\n**Tools** ‚Äî Clone repos, read files, search code, grep for patterns. The LLM decides which tools to use and when, executing them through Anthropic's native tool use API.\n\n| Tool | Purpose | Example use |\n|------|---------|-------------|\n| `clone_and_index` | Clone and index a GitHub repo | \"index pallets/flask\" |\n| `list_repos` | List all indexed repositories | \"what repos do I have?\" |\n| `search_code` | Semantic search over code | \"how does routing work?\" |\n| `read_file` | Read file with line numbers | reading a specific file |\n| `list_directory` | Explore repo structure | \"show me the project layout\" |\n| `grep` | Regex pattern search | \"find all TODO comments\" |\n| `save_memory` | Persist a fact/insight/preference | automatic when discovering patterns |\n| `recall_memory` | Retrieve stored memories | automatic at session start |\n\n**Memory** ‚Äî Persistent JSON storage for facts, insights, and user preferences. Memory is loaded into the system prompt at the start of each session, giving the agent context from previous conversations.\n\nEnables context-aware follow-ups like:\n  > *\"Earlier you found the auth logic in `src/auth/` ‚Äî want me to look for related middleware?\"*\n\n### The Agentic Loop\n\nThe core pattern that makes this work (same as the [Agent Loop](/courses/agentic-ai-engineering/01-foundations/05-agent-loop) tutorial).\n\nThe loop continues until the LLM responds with just text (no tool calls), indicating it has enough information to answer.\n\n### RAG Pipeline\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TB\n  subgraph Indexing[\"üì• Indexing\"]\n    direction LR\n    A[\"üìÅ Repository\"] -->|chunk| B[\"üìÑ Chunks\"]\n    B -->|embed| C[\"üî¢ Vectors\"]\n\n  end\n  C -->|store| D[\"üóÑÔ∏è ChromaDB\"]  \n\n  E[\"üó£Ô∏è User Query\"] -->|embed| F[\"üî¢ Query Vector\"]\n  F -->|similarity search| D\n  D -->|relevant chunks| G[\"üß† LLM\"]\n```\n\n**Chunking strategy**: Python files split on top-level `class`/`def` definitions. Other files split every 50 lines with 10-line overlap. Simple heuristics that work well for educational purposes.\n\n**Embedding model**: `all-MiniLM-L6-v2` via sentence-transformers ‚Äî lightweight, runs locally, no external API needed.\n\n## üèóÔ∏è Code Structure\n\n```\n06-codebase-navigator/\n‚îú‚îÄ‚îÄ 01_codebase_navigator.py             # Main entry point ‚Äî agent + CLI\n‚îú‚îÄ‚îÄ store/\n‚îÇ   ‚îú‚îÄ‚îÄ memory.py                   # JSON-based persistent memory\n‚îÇ   ‚îî‚îÄ‚îÄ vector.py                   # ChromaDB wrapper for embeddings\n‚îú‚îÄ‚îÄ indexer/\n‚îÇ   ‚îú‚îÄ‚îÄ chunker.py                  # File chunking strategies\n‚îÇ   ‚îî‚îÄ‚îÄ embedder.py                 # Sentence-transformer embeddings\n‚îú‚îÄ‚îÄ tools/\n‚îÇ   ‚îú‚îÄ‚îÄ memory.py                   # save_memory, recall_memory\n‚îÇ   ‚îú‚îÄ‚îÄ repo.py                     # clone_and_index, list_repos\n‚îÇ   ‚îú‚îÄ‚îÄ files.py                    # read_file, list_directory\n‚îÇ   ‚îî‚îÄ‚îÄ search.py                   # search_code, grep\n‚îú‚îÄ‚îÄ repos/                          # Cloned repositories (gitignored)\n‚îú‚îÄ‚îÄ data/                           # ChromaDB persistence (gitignored)\n‚îî‚îÄ‚îÄ memory.json                     # Persistent memory (gitignored)\n```\n\n## üí¨ Example Session\n\n```\nCodebase Navigator\n\n> index the flask repo from pallets/flask\n  [tool: clone_and_index] {\"repo\":\"pallets/flask\"}\n  ‚Üí Indexed 'pallets/flask': 142 files, 847 chunks. Ready to search!\n\nNavigator:\nIndexed pallets/flask successfully. What would you like to explore?\n\n> how does routing work?\n  [tool: search_code] {\"query\":\"routing request handling\",\"repo\":\"pallets-flask\"}\n  ‚Üí Search results for: 'routing request handling'\n\nNavigator:\nFlask's routing is built on Werkzeug's routing system. Here's how it works:\n\n1. **Route Registration** (`src/flask/scaffold.py:42`)\n   Routes are registered via the `@app.route()` decorator...\n\n2. **URL Map** (`src/flask/app.py:315`)\n   Flask maintains a Werkzeug `Map` object...\n\n> find all TODO comments\n  [tool: grep] {\"pattern\":\"TODO\",\"repo\":\"pallets-flask\"}\n  ‚Üí Found 3 matches for `TODO`\n\nNavigator:\nFound 3 TODO comments in the Flask codebase:\n- `src/flask/testing.py:89` ‚Äî TODO: deprecate this in 3.1\n...\n```\n\n## ‚ö†Ô∏è Important Considerations\n\n- **Embedding model choice** ‚Äî we use `all-MiniLM-L6-v2` because it's small (~80MB), runs locally with no extra API key, and is good enough for teaching RAG. For production code search, consider a code-specific model (e.g., CodeBERT or OpenAI embeddings)\n- **Embedding model downloads on first run** ‚Äî the model is downloaded once from HuggingFace and cached locally\n- **Large repos take time to index** ‚Äî chunking and embedding hundreds of files requires patience\n- **ChromaDB persists locally** ‚Äî indexed repos are stored in `./data/chroma/` and survive restarts\n- **Memory grows unbounded** ‚Äî in production, you'd want to limit or summarize old memories\n- **No AST parsing** ‚Äî chunking uses simple line-based heuristics, not language-aware parsing\n\n## üëâ Next Steps\n\n- **[Prompt Chaining](/courses/agentic-ai-engineering/02-effective-agents/01-prompt-chaining)** ‚Äî decompose tasks into sequential LLM calls\n- Try indexing multiple repos and asking cross-repo questions\n- Experiment with different embedding models\n- Add new tools (e.g., `run_tests`, `explain_function`)\n- Try different chunking strategies for better search results"},{"title":"Context Management","description":"Handle the reality of finite context windows with sliding windows, summarization, and chunking","slug":"agentic-ai-engineering/03-advanced-techniques/02-context-management","icon":"layers","body":"# Context Management\n\nHandle the reality of finite context windows. Sliding windows, summarization, chunking strategies, and knowing what to keep vs. what to drop.\n\n## üéØ What You'll Learn\n\n- Implement sliding window strategies for long conversations\n- Summarize older context to preserve meaning in fewer tokens\n- Chunk large documents for effective retrieval\n- Prioritize what stays in context when tokens are tight\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Memory","description":"Give agents memory that persists across sessions ‚Äî short-term buffers, long-term stores, and vector databases","slug":"agentic-ai-engineering/03-advanced-techniques/03-memory","icon":"database","body":"# Memory\n\nGive agents memory that persists across sessions. Short-term conversation buffers, long-term vector stores, and hybrid approaches.\n\n## üéØ What You'll Learn\n\n- Build short-term memory for conversation continuity\n- Implement long-term memory with vector stores\n- Design retrieval strategies for relevant memory recall\n- Combine memory types for agents that truly remember\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Structured Output","description":"Force LLM responses into exact schemas ‚Äî JSON mode, Pydantic models, constrained generation","slug":"agentic-ai-engineering/03-advanced-techniques/01-structured-output","icon":"code","body":"# Structured Output\n\nForce LLM responses into exact schemas ‚Äî JSON mode, Pydantic models, constrained generation. The bridge between natural language and your application code.\n\n## üéØ What You'll Learn\n\n- Use JSON mode and response schemas across providers\n- Validate LLM output with Pydantic models\n- Handle schema violations and malformed responses\n- Design schemas that models can reliably follow\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Prompt Caching","description":"Cache static prompt prefixes to cut latency and cost","slug":"agentic-ai-engineering/03-advanced-techniques/04-prompt-caching","icon":"zap","body":"# Prompt Caching\n\nCache static prompt prefixes to cut latency and cost. Understand when caching helps, when it doesn't, and how to structure prompts for maximum reuse.\n\n## üéØ What You'll Learn\n\n- Enable prompt caching across different providers\n- Structure prompts to maximize cache hits\n- Measure cost savings and latency improvements\n- Know when caching helps vs. adds unnecessary complexity\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Agent Loop","description":"Build autonomous agents that use tools iteratively to accomplish complex goals","slug":"agentic-ai-engineering/01-foundations/05-agent-loop","icon":"repeat","body":"# Agent Loop\n\nLearn how to build autonomous coding agents that use tools in a loop to complete tasks. This tutorial demonstrates the core pattern behind AI agents: iteratively calling an LLM, executing requested tools, and feeding results back until the task is complete.\n\n## üéØ What You'll Learn\n\n- Implement the core agent loop pattern (LLM call ‚Üí tool execution ‚Üí result feedback)\n- Build coding agents with filesystem tools (read_file, write_file, bash)\n- Handle tool calls and results in conversation flow\n- Build interactive CLI agents with proper error handling\n\n## üì¶ Available Examples\n\n| Provider                                        | File                                                         | Description                                                        |\n| ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------ |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_minimal_agent.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/05-agent-loop/01_minimal_agent.py)                   | Minimal agent loop (~55 lines) with human-in-the-loop confirmation |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [02_coding_agent_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/05-agent-loop/02_coding_agent_anthropic.py) | Full coding agent using Claude Messages API                        |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [03_coding_agent_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/05-agent-loop/03_coding_agent_openai.py)       | Coding agent using OpenAI Responses API                            |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 01-foundations/05-agent-loop python {script_name}\n\n# Example\nuv run --directory 01-foundations/05-agent-loop python 01_minimal_agent.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### 1. The Agent Loop Pattern\n\nThe core pattern is simple: call the LLM, execute any requested tools, feed results back, repeat until done.\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A([\"üó£Ô∏è User Task        \"]) -->|init| B[\"üß† LLM Call      \"]\n    B -->|evaluate| C{\"‚öôÔ∏è Tool Calls?     \"}\n    C -->|\"no tools\"| D([\"üìÑ Return Response  \"])\n    C -->|\"has tools\"| E[\"üîß Execute Tools    \"]\n    E -->|collect| F[\"üìù Append Results   \"]\n    F -->|iterate| B\n```\n\n```python\nwhile iteration < max_iterations:\n    # 1. Call the model with tools\n    response = client.messages.create(\n        model=model,\n        tools=TOOLS,\n        messages=messages,\n    )\n\n    # 2. If no tool calls, task is complete\n    if response.stop_reason == \"end_turn\":\n        return response.content[0].text\n\n    # 3. Execute tools and collect results\n    for tool_call in response.tool_calls:\n        result = execute_tool(tool_call.name, tool_call.input)\n        tool_results.append(result)\n\n    # 4. Add results to conversation and continue\n    messages.append(tool_results)\n```\n\n### 2. Tools\n\nTool definitions and execution are covered in [Tool Use](/courses/agentic-ai-engineering/01-foundations/04-tool-use). This tutorial uses three tools: `read_file`, `write_file`, and `bash`.\n\n### 3. Appending Tool Results\n\n**Anthropic** - Append assistant response and tool results as messages:\n```python\nmessages.append({\"role\": \"assistant\", \"content\": response.content})\nmessages.append({\"role\": \"user\", \"content\": tool_results})\n```\n\n**OpenAI Responses API** - Pass tool outputs as `input` with `previous_response_id`:\n```python\ntool_outputs = [\n    {\n        \"type\": \"function_call_output\",\n        \"call_id\": call.call_id,\n        \"output\": json.dumps({\"result\": result}),\n    }\n    for call, result in zip(function_calls, results)\n]\nresponse = client.responses.create(\n    model=model,\n    tools=TOOLS,\n    input=tool_outputs,\n    previous_response_id=response.id,\n)\n```\n\n## üèóÔ∏è Code Structure\n\nBoth examples follow a consistent structure:\n\n```python\nSYSTEM_PROMPT = \"\"\"You are a coding agent...\"\"\"\n\nTOOLS = [...]  # Tool definitions\n\ndef execute_tool(name: str, tool_input: dict) -> str:\n    \"\"\"Execute a tool and return the result.\"\"\"\n    ...\n\nclass CodingAgent:\n    \"\"\"Autonomous agent that uses tools in a loop.\"\"\"\n\n    def __init__(self, model: str):\n        self.client = ...\n        self.model = model\n        self.max_iterations = 10\n\n    def run(self, task: str) -> str:\n        \"\"\"Execute the agent loop for the given task.\"\"\"\n        # Agent loop implementation\n        ...\n\ndef main() -> None:\n    \"\"\"Interactive CLI with welcome message.\"\"\"\n    agent = CodingAgent()\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in (\"exit\", \"quit\", \"q\"):\n            break\n        response = agent.run(user_input)\n        print(f\"Agent: {response}\")\n```\n\n\n## üëâ Next Steps\n\nOnce you've mastered the agent loop pattern:\n- Add more tools (web search, database queries, API calls)\n- Implement tool confirmation for destructive actions\n- Add memory/context management for longer conversations\n- Explore streaming responses for better UX"},{"title":"Interactive Chat","description":"Build an interactive chat loop with message history using Anthropic Claude and OpenAI GPT","slug":"agentic-ai-engineering/01-foundations/03-chat","icon":"message-circle","body":"# Interactive Chat\n\nBuild an interactive chat application with conversation history management. This tutorial demonstrates how to maintain context across multiple turns and create an engaging conversational user experience.\n\n## üéØ What You'll Learn\n\n- Implement an interactive chat loop with user input\n- Manage conversation history across multiple turns\n- Maintain context for natural multi-turn conversations\n- Track token usage and conversation statistics\n- Create rich console output for better UX\n\n## üì¶ Available Examples\n\n| Provider                                        | File                                         | Description                                 |\n| ----------------------------------------------- | -------------------------------------------- | ------------------------------------------- |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_chat_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/03-chat/01_chat_anthropic.py) | Interactive chat using Claude Messages API  |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [02_chat_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/03-chat/02_chat_openai.py)       | Interactive chat using OpenAI Responses API |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 01-foundations/03-chat python {script_name}\n\n# Example\nuv run --directory 01-foundations/03-chat python 01_chat_anthropic.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### 1. Chat Loop Pattern\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A([\"üó£Ô∏è User Input      \"]) -->|append| B[\"üìù Store in History \"]\n    B -->|send| C[\"üß† LLM Call       \"]\n    C -->|append| D[\"üìù Store Response   \"]\n    D -->|render| E([\"üí¨ Display Output   \"])\n    E -->|loop| A\n```\n\n### 2. Message History Management\n\nThe key to multi-turn conversations is maintaining a message history array:\n\n**Anthropic:**\n```python\nclass ChatSession:\n    def __init__(self, model: str):\n        self.client = anthropic.Anthropic()\n        self.messages: list[dict[str, str]] = []\n        self.model = model\n\n    def send_message(self, user_message: str) -> str:\n        # Add user message to history\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n\n        # Send entire history to API\n        response = self.client.messages.create(\n            model=self.model,\n            messages=self.messages,\n        )\n\n        # Extract response\n        assistant_message = response.content[0].text\n\n        # Add assistant response to history\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n\n        return assistant_message\n```\n\n**OpenAI:**\n```python\nclass ChatSession:\n    def __init__(self, model: str):\n        self.client = OpenAI()\n        self.messages: list[dict[str, str]] = []\n        self.model = model\n\n    def send_message(self, user_message: str) -> str:\n        # Add user message to history\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n\n        # Send entire history to API using Responses API\n        response = self.client.responses.create(\n            model=self.model,\n            input=self.messages,\n        )\n\n        # Extract response\n        assistant_message = response.output_text or \"\"\n\n        # Add assistant response to history\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n\n        return assistant_message\n```\n\n### 3. Interactive Chat Loop\n\nCreate a continuous conversation flow:\n\n```python\ndef main() -> None:\n    console = Console()\n    chat = ChatSession(\"model-name\")\n\n    # Welcome message\n    console.print(Panel(\"Welcome to Chat!\"))\n\n    # Interactive loop\n    while True:\n        # Get user input\n        console.print(\"You: \", end=\"\")\n        user_input = input().strip()\n\n        # Exit condition\n        if user_input.lower() in [\"quit\", \"exit\", \"\"]:\n            break\n\n        # Process message\n        try:\n            response = chat.send_message(user_input)\n            console.print(f\"Assistant: {response}\")\n        except Exception as e:\n            console.print(f\"Error: {e}\")\n            break\n```\n\n### 4. Token Tracking\n\nMonitor API usage across the conversation:\n\n**Anthropic:**\n```python\ntoken_tracker = AnthropicTokenTracker()\n\n# After each API call\nresponse = self.client.messages.create(...)\ntoken_tracker.track(response.usage)\n\n# At end of session\ntoken_tracker.report()  # Shows total input/output/cost\n```\n\n**OpenAI:**\n```python\ntoken_tracker = OpenAITokenTracker()\n\n# After each API call\nresponse = self.client.responses.create(...)\ntoken_tracker.track(response.usage)\n\n# At end of session\ntoken_tracker.report()  # Shows total input/output/cost\n```\n\n## ‚ö†Ô∏è Important Considerations\n\n**Context Window Limits**: As conversations grow, the message history consumes more tokens. Eventually, you'll hit the model's context window limit. Advanced techniques for handling this include:\n- Truncating old messages\n- Summarizing conversation history\n- Using sliding windows\n\n**Error Handling**: Production chat applications should handle:\n- Network errors and API failures\n- Rate limiting and retries\n- Invalid user input\n- Token limit exceeded errors\n\n**Cost Management**: Every message sends the entire conversation history. Longer conversations = higher costs per message. Monitor token usage carefully.\n\nThese strategies will be covered in future tutorials.\n\n## üëâ Next Steps\n\nOnce you've built interactive chat sessions, continue to:\n- **[Tool Use](/courses/agentic-ai-engineering/01-foundations/04-tool-use)** - Add external capabilities to your chat agent\n- **Experiment** - Try different conversation flows and system prompts\n- **Enhance** - Add features like conversation summarization or history persistence"},{"title":"Simple LLM Call","description":"Make your first API call using Anthropic Claude, OpenAI GPT, and LiteLLM","slug":"agentic-ai-engineering/01-foundations/01-simple-llm-call","icon":"zap","body":"# Simple LLM Call\n\nLearn how to make basic calls to LLM APIs. This tutorial demonstrates how to interact with different LLM providers and get a simple text response.\n\n## üéØ What You'll Learn\n\n- Initialize and configure LLM clients for different providers\n- Make simple API calls with single prompts\n- Extract text responses from API calls\n- Use a unified interface (LiteLLM) to work with multiple providers\n\n## üì¶ Available Examples\n\n| Provider                                        | File                                                 | Description                        |\n| ----------------------------------------------- | ---------------------------------------------------- | ---------------------------------- |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_llm_call_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/01-simple-llm-call/01_llm_call_anthropic.py) | Basic Claude Messages API calls    |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [02_llm_call_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/01-simple-llm-call/02_llm_call_openai.py)       | Basic OpenAI Responses API calls   |\n| ![LiteLLM](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/litellm.svg)     | [03_llm_call_litellm.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/01-simple-llm-call/03_llm_call_litellm.py)     | Unified interface for any provider |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 01-foundations/01-simple-llm-call python {script_name}\n\n# Example\nuv run --directory 01-foundations/01-simple-llm-call python 01_llm_call_anthropic.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### 1. Simple LLM Call Flow\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart LR\n    A([\"‚ö° Input Prompt\"]) -->|request| B[\"üß† LLM Call   \"]\n    B -->|response| C([\"üìÑ Response Text\"])\n```\n\n### 2. LLM Client Initialization\n\nEach provider has its own client initialization:\n\n**Anthropic:**\n```python\nimport anthropic\n\nclient = anthropic.Anthropic()  # Uses ANTHROPIC_API_KEY from env\nmodel = \"claude-sonnet-4-5-20250929\"\n```\n\n**OpenAI:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()  # Uses OPENAI_API_KEY from env\nmodel = \"gpt-4.1\"\n```\n\n**LiteLLM:**\n```python\nfrom litellm import completion\n\n# No client needed - just call completion()\n# Uses appropriate API key based on model name\n```\n\n### 3. Making API Calls\n\n**Anthropic (Messages API):**\n```python\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    temperature=0.1,\n    max_tokens=1024,\n    system=\"You are a helpful AI assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n)\ntext = response.content[0].text\n```\n\n**OpenAI (Responses API):**\n```python\nresponse = client.responses.create(\n    model=\"gpt-4.1\",\n    temperature=0.1,\n    max_output_tokens=1024,\n    instructions=\"You are a helpful AI assistant.\",\n    input=\"Hello!\",\n)\ntext = response.output_text\n```\n\n**LiteLLM (Unified API):**\n```python\nresponse = completion(\n    model=\"gpt-4.1\",  # Or \"claude-sonnet-4-5-20250929\"\n    temperature=0.1,\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n)\ntext = response.choices[0].message.content\n```\n\n> These examples show basic (non-streaming) API calls. For streaming responses, see the [Anthropic Streaming docs](https://docs.anthropic.com/en/api/messages-streaming), [OpenAI Streaming docs](https://platform.openai.com/docs/api-reference/streaming), and [LiteLLM Streaming docs](https://docs.litellm.ai/docs/completion/stream).\n\n### 4. Key Configuration Parameters\n\n**Model**: Specifies which LLM to use (e.g., `claude-sonnet-4-5-20250929`, `gpt-4.1`)\n\n**Temperature**: Controls randomness (0.0 = deterministic, 1.0 = creative)\n- Lower values (0.0-0.3) for factual, consistent responses\n- Higher values (0.7-1.0) for creative, varied outputs\n\n**Max Tokens**: Limits the response length\n- Anthropic/LiteLLM: `max_tokens`\n- OpenAI Responses API: `max_output_tokens`\n\n**System Prompt**: Defines the assistant's behavior and context\n- Anthropic: `system` parameter\n- OpenAI: `instructions` parameter\n- LiteLLM: System message in `messages` array\n\n> Other advanced parameters like `top_p`, `top_k`, `stop_sequences`, `presence_penalty`, `frequency_penalty`, and `seed` will be covered in future tutorials.\n\n## üèóÔ∏è Code Structure\n\nAll examples follow a consistent structure:\n\n```python\nclass LLMClient:\n    \"\"\"Encapsulates LLM interaction logic.\"\"\"\n\n    def __init__(self, model: str):\n        self.client = ...  # Initialize API client\n        self.model = model\n        self.system_prompt = \"...\"\n\n    def run(self, prompt: str) -> str:\n        \"\"\"Execute a single LLM call.\"\"\"\n        # 1. Make API call\n        response = self.client...\n\n        # 2. Extract and return text\n        return response_text\n\n\ndef main() -> None:\n    \"\"\"Orchestrates execution flow.\"\"\"\n    # 1. Initialize client\n    client = LLMClient(\"model-name\")\n\n    # 2. Define prompt\n    prompt = \"...\"\n\n    # 3. Get response\n    response = client.run(prompt)\n\n    # 4. Display result\n    logger.info(f\"Response: {response}\")\n```\n\n## üëâ Next Steps\n\nOnce you've mastered simple LLM calls, continue to:\n- **[Prompt Engineering](/courses/agentic-ai-engineering/01-foundations/02-prompt-engineering)** - Learn to craft effective prompts for better responses\n- **Experiment** - Try different models, temperatures, and prompts\n- **Explore** - Modify the examples to add features like retry logic or error handling"},{"title":"Tool Use","description":"Enable LLMs to call functions and interact with external systems","slug":"agentic-ai-engineering/01-foundations/04-tool-use","icon":"wrench","body":"# Tool Use\n\nLearn how to give LLMs the ability to call functions (tools) to interact with the real world. This tutorial demonstrates how to define tools, handle tool calls, and execute functions on behalf of the model.\n\n## üéØ What You'll Learn\n\n- Define tools with JSON Schema for LLM consumption\n- Handle the tool call loop (request -> execute -> respond)\n- Execute functions safely with guardrails\n- Work with multiple tool calls in a single response\n\n## üì¶ Available Examples\n\n| Provider                                        | File                                                 | Description                        |\n| ----------------------------------------------- | ---------------------------------------------------- | ---------------------------------- |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_tool_use_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/04-tool-use/01_tool_use_anthropic.py) | Tool use with Claude Messages API  |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [02_tool_use_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/04-tool-use/02_tool_use_openai.py)       | Tool use with OpenAI Responses API |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 01-foundations/04-tool-use python {script_name}\n\n# Example\nuv run --directory 01-foundations/04-tool-use python 01_tool_use_anthropic.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### 1. Tool Definition\n\nTools are defined using JSON Schema so the LLM understands what functions are available:\n\n**Anthropic:**\n```python\nTOOLS = [\n    {\n        \"name\": \"calculator\",\n        \"description\": \"Performs basic arithmetic operations.\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"operation\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n                },\n                \"a\": {\"type\": \"number\"},\n                \"b\": {\"type\": \"number\"},\n            },\n            \"required\": [\"operation\", \"a\", \"b\"],\n        },\n    },\n]\n```\n\n**OpenAI:**\n```python\nTOOLS = [\n    {\n        \"type\": \"function\",\n        \"name\": \"calculator\",\n        \"description\": \"Performs basic arithmetic operations.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"operation\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n                },\n                \"a\": {\"type\": \"number\"},\n                \"b\": {\"type\": \"number\"},\n            },\n            \"required\": [\"operation\", \"a\", \"b\"],\n        },\n    },\n]\n```\n\n### 2. The Tool Call Loop\n\nThe LLM doesn't execute tools directly - it requests tool calls that you execute:\n\n```\nUser Message\n    |\nLLM Response (with tool_use)\n    |\nExecute Tool -> Get Result\n    |\nSend Result Back to LLM\n    |\nLLM Response (final answer)\n```\n\n**Anthropic:**\n```python\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=TOOLS,\n    messages=messages,\n)\n\nif response.stop_reason == \"tool_use\":\n    for block in response.content:\n        if isinstance(block, ToolUseBlock):\n            result = execute_tool(block.name, block.input)\n            # Send result back with tool_use_id\n```\n\n**OpenAI:**\n```python\nresponse = client.responses.create(\n    model=\"gpt-4.1\",\n    tools=TOOLS,\n    input=messages,\n)\n\nfor output in response.output:\n    if output.type == \"function_call\":\n        result = execute_tool(output.name, json.loads(output.arguments))\n        # Send result back with call_id\n```\n\n### 3. Tool Implementation with Guardrails\n\nAlways validate and sanitize tool inputs, especially for system-level tools:\n\n```python\nBLOCKED_COMMANDS = [\"rm\", \"sudo\", \"chmod\", \"shutdown\", \">\", \">>\"]\n\ndef run_bash(command: str, timeout: int = 30) -> dict:\n    \"\"\"Execute a bash command with safety guardrails.\"\"\"\n    # Block dangerous commands\n    for blocked in BLOCKED_COMMANDS:\n        if blocked in command.lower():\n            return {\"error\": f\"Command blocked: contains '{blocked}'\"}\n\n    result = subprocess.run(\n        command,\n        shell=True,\n        capture_output=True,\n        timeout=timeout,\n    )\n    return {\"stdout\": result.stdout, \"stderr\": result.stderr}\n```\n\n### 4. Handling Multiple Tool Calls\n\nLLMs can request multiple tool calls in a single response. Process all of them before continuing:\n\n**Anthropic:**\n```python\ntool_results = []\nfor tool_use in tool_uses:\n    result = execute_tool(tool_use.name, tool_use.input)\n    tool_results.append({\n        \"type\": \"tool_result\",\n        \"tool_use_id\": tool_use.id,\n        \"content\": json.dumps(result),\n    })\nmessages.append({\"role\": \"user\", \"content\": tool_results})\n```\n\n**OpenAI:**\n```python\n# Add function calls to messages first\nmessages.extend(response.output)\n\n# Then add results\nfor func_call in function_calls:\n    result = execute_tool(func_call.name, json.loads(func_call.arguments))\n    messages.append({\n        \"type\": \"function_call_output\",\n        \"call_id\": func_call.call_id,\n        \"output\": json.dumps(result),\n    })\n```\n\n## üß∞ Tools in This Tutorial\n\n| Tool         | Description                                        |\n| ------------ | -------------------------------------------------- |\n| `calculator` | Basic arithmetic (add, subtract, multiply, divide) |\n| `read_file`  | Read file contents from the filesystem             |\n| `run_bash`   | Execute shell commands (with safety guardrails)    |\n\n## üèóÔ∏è Code Structure\n\nBoth examples follow a consistent structure:\n\n```python\n# 1. Define tools as JSON Schema\nTOOLS = [...]\n\n# 2. Implement tool functions\ndef calculator(operation: str, a: float, b: float) -> dict:\n    ...\n\ndef read_file(path: str) -> dict:\n    ...\n\ndef run_bash(command: str) -> dict:\n    ...\n\n# 3. Tool execution dispatcher\nTOOL_FUNCTIONS = {\"calculator\": calculator, \"read_file\": read_file, ...}\n\ndef execute_tool(name: str, input: dict) -> Any:\n    return TOOL_FUNCTIONS[name](**input)\n\n\n# 4. Chat class with tool loop\nclass ToolUseChat:\n    def send_message(self, message: str) -> str:\n        while True:\n            response = self.client.create(tools=TOOLS, ...)\n\n            if has_tool_calls(response):\n                execute_tools_and_add_results()\n                continue\n            else:\n                return response.text\n\n\n# 5. Main orchestration\ndef main():\n    chat = ToolUseChat(model, token_tracker, console)\n    while True:\n        user_input = input()\n        response = chat.send_message(user_input)\n        print(response)\n```\n\n## üëâ Next Steps\n\nOnce you've mastered tool use, continue to:\n- **[Agent Loop](/courses/agentic-ai-engineering/01-foundations/05-agent-loop)** - Build autonomous agents that use tools to complete tasks\n- **Experiment** - Add more tools like web search, database queries, or API calls\n- **Explore** - Implement tool choice modes (`auto`, `required`, `none`)"},{"title":"Prompt Engineering","description":"Learn prompt engineering techniques including system messages, few-shot examples, and structured output","slug":"agentic-ai-engineering/01-foundations/02-prompt-engineering","icon":"wand","body":"# Prompt Engineering\n\nLearn how to shape LLM behavior through prompting techniques. Every AI agent's capabilities start with how its prompts are engineered ‚Äî this tutorial covers the core techniques you'll use in every agent you build.\n\n## üéØ What You'll Learn\n\n- Use system prompts and role engineering to control LLM behavior\n- Apply few-shot prompting for in-context learning\n- Guide reasoning with chain-of-thought (CoT) prompting\n- Extract structured JSON output via prompt instructions\n- Use provider-specific techniques: Anthropic XML scaffolding + prefill, OpenAI JSON schema enforcement\n- Compare prompting strategies side-by-side to understand their trade-offs\n\n## üì¶ Available Examples\n\n| Provider                                        | File                                                                   | Description                                                   |\n| ----------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------- |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_system_prompts_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/01_system_prompts_anthropic.py)       | System prompts & role engineering                             |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [02_system_prompts_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/02_system_prompts_openai.py)             | System prompts & role engineering                             |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [03_few_shot_cot_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/03_few_shot_cot_anthropic.py)           | Zero-shot, few-shot & chain-of-thought demos                  |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [04_few_shot_cot_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/04_few_shot_cot_openai.py)                 | Zero-shot, few-shot & chain-of-thought demos                  |\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [05_structured_output_anthropic.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/05_structured_output_anthropic.py) | Product extraction ‚Äî prompt, XML prefill & native schema      |\n| ![OpenAI](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/openai.svg)       | [06_structured_output_openai.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/01-foundations/02-prompt-engineering/06_structured_output_openai.py)       | Product extraction ‚Äî prompt, scaffolding & schema enforcement |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 01-foundations/02-prompt-engineering python {script_name}\n\n# Example\nuv run --directory 01-foundations/02-prompt-engineering python 01_system_prompts_anthropic.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### 1. Prompt Engineering Layers\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart LR\n    A([\"üó£Ô∏è System Prompt \"]) -->|shape behavior| D[\"üß† LLM Call   \"]\n    B([\"üìù Few-Shot Examples\"]) -->|teach patterns| D\n    C([\"üìã Output Schema \"]) -->|constrain format| D\n    D -->|response| E([\"üìÑ Structured Output\"])\n```\n\nEach layer adds more control over the LLM's response. Used together, they let you build agents that produce reliable, parseable output.\n\n### 2. System Prompts & Role Engineering\n\nSystem prompts are the primary lever for controlling agent behavior. The scripts compare three levels of refinement on the same support ticket triage task ‚Äî an ambiguous ticket forces each prompt to determine *how* the ticket is interpreted and *what* gets prioritized:\n\n| Configuration                   | What It Does                                                            |\n| ------------------------------- | ----------------------------------------------------------------------- |\n| **Generic assistant**           | Baseline ‚Äî \"You are a helpful assistant\" (hedges, gives generic advice) |\n| **Role-assigned expert**        | Identity + domain expertise + decisiveness (makes a call)               |\n| **Role + constraints + format** | All of the above + strict output sections (terse, actionable)           |\n\n**Anthropic** ‚Äî system prompt as a top-level parameter:\n```python\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    system=\"You are a senior support engineer at a SaaS company...\",  # System prompt\n    messages=[{\"role\": \"user\", \"content\": \"Analyze this support ticket...\"}],\n)\n```\n\n**OpenAI** ‚Äî system prompt via `instructions`:\n```python\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    instructions=\"You are a senior support engineer at a SaaS company...\",  # System prompt\n    input=\"Analyze this support ticket...\",\n)\n```\n\n> The more specific and constrained your system prompt, the more consistent and useful the output. This is the single most important prompt engineering technique for agents.\n\n### 3. Few-Shot & Chain-of-Thought\n\nThe scripts demonstrate three techniques, each on a task where it shines ‚Äî showing *why* you'd choose one over another:\n\n| Technique     | Demo Task                   | Why This Technique                             |\n| ------------- | --------------------------- | ---------------------------------------------- |\n| **Zero-shot** | Sentiment analysis          | Model already knows POSITIVE/NEGATIVE/NEUTRAL  |\n| **Few-shot**  | Custom label classification | Teaches domain labels like `BILLING_DISPUTE`   |\n| **CoT**       | Root cause analysis         | Multi-step reasoning produces better diagnosis |\n\n**Zero-shot** ‚Äî no examples needed when the task is well-understood:\n```python\nsystem = (\n    \"Classify the sentiment of the following product review.\\n\"\n    \"Respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\"\n)\n```\n\n**Few-shot** ‚Äî examples teach the model YOUR custom taxonomy:\n```python\nEXAMPLES = [\n    (\"I was charged twice for the same subscription\", \"BILLING_DISPUTE\"),\n    (\"Can't log in even after resetting my password\", \"ACCOUNT_ACCESS\"),\n]\n\nexamples_text = \"\\n\".join(\n    f'Ticket: \"{text}\"\\nCategory: {label}' for text, label in EXAMPLES\n)\n```\n\n**Chain-of-thought** ‚Äî step-by-step reasoning for complex problems:\n```python\nsystem = (\n    \"Analyze this bug report step by step:\\n\"\n    \"1. What patterns do you observe? (timing, scope, triggers)\\n\"\n    \"2. What does each clue rule in or rule out?\\n\"\n    \"3. What is the most likely root cause?\\n\"\n    \"4. What would you check first to confirm?\"\n)\n```\n\n> **When to use what:** Zero-shot for well-known tasks (fast, cheap). Few-shot when you need custom labels or domain-specific classification (more input tokens). CoT when accuracy on reasoning tasks matters more than speed (more output tokens).\n\n### 4. Structured Output & Scaffolding\n\nAgents must produce parseable output. The scripts extract product data from a single description using three methods each, making it easy to compare reliability:\n\n```python\n# Product extraction schema used across all methods\nclass ProductExtraction(BaseModel):\n    name: str\n    category: str\n    price: float\n    features: list[str]\n    in_stock: bool\n```\n\n**Anthropic ‚Äî native JSON schema via `output_config` (recommended):**\n```python\n# API-level schema enforcement ‚Äî guaranteed valid JSON\nresponse = client.messages.parse(\n    model=\"claude-sonnet-4-20250514\",\n    messages=[{\"role\": \"user\", \"content\": product_description}],\n    output_format=ProductExtraction,\n)\nproduct = response.parsed_output  # Validated Pydantic model instance\n```\n\n**Anthropic ‚Äî XML scaffolding + assistant prefill (prompting technique):**\n```python\n# XML tags structure the input, prefill forces JSON start\nmessages = [\n    {\"role\": \"user\", \"content\": \"<schema>...</schema>\\n<product_description>...</product_description>\"},\n    {\"role\": \"assistant\", \"content\": \"{\"},  # Prefill technique\n]\n```\n\n**OpenAI ‚Äî native JSON schema enforcement:**\n```python\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    instructions=\"Extract product information...\",\n    input=product_description,\n    text={\"format\": {\n        \"type\": \"json_schema\",\n        \"name\": \"product_extraction\",\n        \"strict\": True,\n        \"schema\": { ... }\n    }},\n)\n```\n\n> **Both providers now offer schema enforcement.** Anthropic's `output_config` and OpenAI's `text.format` both guarantee valid JSON via constrained decoding. Prompt-based techniques (prefill, XML scaffolding) remain useful for older models or when you need more control over the prompting strategy.\n\n### 5. Output Validation\n\nAlways validate structured output for prompt-based methods. Native schema enforcement handles validation automatically, but check for refusals (`stop_reason: \"refusal\"`) and token limits (`stop_reason: \"max_tokens\"`) which can produce non-conforming output:\n\n```python\ndef try_parse_json(raw: str) -> dict | None:\n    text = raw.strip()\n    # Strip markdown fences if the LLM added them\n    if text.startswith(\"```\"):\n        lines = text.splitlines()\n        text = \"\\n\".join(lines[1:-1])\n    try:\n        return json.loads(text)\n    except json.JSONDecodeError:\n        return None\n```\n\n## ‚ö†Ô∏è Important Considerations\n\n- **Prompt injection** ‚Äî System prompts can be overridden by adversarial user input. Never rely solely on prompts for security boundaries. This becomes critical in [Tool Use](/courses/agentic-ai-engineering/01-foundations/04-tool-use).\n- **Token costs** ‚Äî Few-shot examples add input tokens to every call. In high-volume agents, consider whether the accuracy improvement justifies the cost.\n- **JSON reliability** ‚Äî Prompt-based JSON extraction can fail. Use provider-native schema enforcement (Anthropic `output_config`, OpenAI `text.format`) for guaranteed valid JSON in production.\n- **Temperature** ‚Äî Set `temperature=0.0` for classification and structured output tasks where consistency matters. These scripts all use low temperature for reproducible results.\n\n## üëâ Next Steps\n\nOnce you've mastered prompt engineering, continue to:\n- **[Chat](/courses/agentic-ai-engineering/01-foundations/03-chat)** ‚Äî Add conversation history and multi-turn interactions\n- **Experiment** ‚Äî Try different role descriptions, add more few-shot examples, or combine techniques across scripts\n- **Explore** ‚Äî Modify the classification categories or task schema to match your domain"},{"title":"Multimodal","description":"Process images, audio, and files alongside text","slug":"agentic-ai-engineering/03-advanced-techniques/05-multimodal","icon":"image","body":"# Multimodal\n\nProcess images, audio, and files alongside text. Build agents that can see screenshots, read documents, and work with the real-world data your users have.\n\n## üéØ What You'll Learn\n\n- Send images and screenshots to vision-capable models\n- Process audio input and generate audio output\n- Handle PDFs, documents, and file uploads\n- Build agents that work with real-world multimodal data\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"MCP (Model Context Protocol)","description":"Connect agents to external tools through a standardized protocol","slug":"agentic-ai-engineering/03-advanced-techniques/06-mcp","icon":"plug","body":"# MCP (Model Context Protocol)\n\nConnect agents to external tools through a standardized protocol. Build and consume MCP servers for databases, APIs, file systems, and more.\n\n## üéØ What You'll Learn\n\n- Understand the Model Context Protocol specification\n- Consume existing MCP servers for common tools\n- Build custom MCP servers for your own APIs\n- Integrate MCP tools into your agent architecture\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"RAG Techniques","description":"Advanced retrieval patterns ‚Äî vector search, hybrid retrieval, GraphRAG, and agentic RAG","slug":"agentic-ai-engineering/03-advanced-techniques/08-rag-techniques","icon":"search","body":"# RAG Techniques\n\nMove beyond basic vector similarity search. Learn hybrid retrieval, knowledge graphs, contextual chunking, and agentic RAG patterns that handle complex, multi-hop queries.\n\n## üéØ What You'll Learn\n\n- Implement hybrid search combining keyword (BM25) and semantic retrieval\n- Use re-ranking to filter and prioritize retrieved context\n- Build GraphRAG pipelines with knowledge graphs for relationship-aware retrieval\n- Design agentic RAG where the agent decides when and what to retrieve\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Multi-Agent Systems","description":"Multiple agents collaborating on shared tasks with communication and delegation","slug":"agentic-ai-engineering/03-advanced-techniques/07-multi-agent-systems","icon":"users","body":"# Multi-Agent Systems\n\nMultiple agents collaborating on shared tasks. Communication patterns, delegation, conflict resolution, and when multi-agent is (and isn't) the right call.\n\n## üéØ What You'll Learn\n\n- Design agent communication and message passing patterns\n- Implement delegation and handoff between agents\n- Handle conflicts when agents disagree\n- Know when multi-agent adds value vs. unnecessary complexity\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Prompt Chaining","description":"Decompose tasks into sequential LLM calls where each step builds on the previous","slug":"agentic-ai-engineering/02-effective-agents/01-prompt-chaining","icon":"link","body":"# Prompt Chaining ‚Äî The Blog Assembly Line\n\nDecompose a task into a sequence of fixed steps, where each LLM call processes the output of the previous one. Simple, linear, and predictable ‚Äî though individual steps can use tools like web search for grounded output.\n\n## üéØ What You'll Learn\n\n- Design multi-step LLM pipelines with clear handoffs between stages\n- Pass context and results between sequential calls using focused prompts\n- Debug and trace execution through the chain with per-step callbacks and token tracking\n- Know when chaining beats a single complex prompt\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_prompt_chaining.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/01-prompt-chaining/01_prompt_chaining.py) | Blog post assembly line with 3-step chain |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/01-prompt-chaining python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/01-prompt-chaining python 01_prompt_chaining.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### Sequential Pipeline\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart LR\n    A[\"üó£Ô∏è Topic     \"] -->|request| B[\"üß† Outliner / Sonnet     \"]\n    B -->|outline| C[\"üß† Writer / Haiku + üîç     \"]\n    C -->|draft| D[\"üß† Editor / Sonnet     \"]\n    D -->|polished| E[\"üìÑ Final Post     \"]\n```\n\nEach step has a focused system prompt and a single responsibility. The output of one step becomes the input of the next.\n\n### Quality Gates\n\nBetween steps, validate that the previous step produced usable output. If the outliner returns empty text, abort early rather than sending garbage downstream.\n\n### Dual Model Strategy\n\nThe chain uses different models for different steps based on task complexity:\n\n- **Sonnet** (steps 1 & 3): Outlining and editing require nuance and judgment\n- **Haiku** (step 2): Writing from a structured outline is more straightforward ‚Äî a faster, cheaper model works well\n\nThis is a practical cost optimization: use the most capable model only where it matters.\n\n### Web Search in the Chain\n\nStep 2 (Writer) has access to Anthropic's built-in web search tool, limited to 3 uses per run:\n\n```python\nWEB_SEARCH_TOOL = {\"type\": \"web_search_20250305\", \"name\": \"web_search\", \"max_uses\": 3}\n```\n\nThe model decides autonomously whether to search ‚Äî the system prompt says \"use web search if the topic would benefit from current information.\" This keeps the chain simple (no explicit search logic) while enabling grounded, up-to-date content.\n\n### Step Design\n\n- **Outliner** (Sonnet): Generate structure (title + 5 bullet points)\n- **Writer** (Haiku + web search): Expand outline into full blog post, 2-3 paragraphs per section\n- **Editor** (Sonnet): Polish grammar, clarity, and flow; add a Key Takeaways section\n\nEach prompt is optimized for its specific task ‚Äî not a single \"do everything\" prompt.\n\n### When to Chain vs. Single Prompt\n\nChaining adds complexity ‚Äî use it when the benefits outweigh the cost:\n\n- **Chain when** steps have different requirements (models, tools, temperature), when intermediate output needs validation, or when debugging requires visibility into each stage\n- **Single prompt when** the task is straightforward enough that one well-crafted prompt handles it reliably ‚Äî adding steps just adds latency and failure points\n\nIn this tutorial, chaining wins because each step genuinely benefits from a different setup: the outliner needs precision (Sonnet), the writer needs web access (Haiku + search), and the editor needs judgment (Sonnet).\n\n## ‚ö†Ô∏è Important Considerations\n\n- Chain length matters ‚Äî each step adds latency and token cost\n- Errors compound: a bad outline produces a bad article no matter how good the writer prompt is\n- Web search adds latency and non-determinism ‚Äî the same topic may produce different articles based on search results\n- Consider adding validation/gates between steps for production use\n\n## üëâ Next Steps\n\n- [02 - Routing](/courses/agentic-ai-engineering/02-effective-agents/02-routing) ‚Äî add input classification to route to specialized chains\n- Experiment: add a 4th step (e.g., SEO optimizer or fact-checker)"},{"title":"Routing","description":"Classify incoming requests and dispatch them to specialized handlers","slug":"agentic-ai-engineering/02-effective-agents/02-routing","icon":"split","body":"# Routing ‚Äî The Content Strategist\n\nRoute to specialized handlers based on content analysis. The router picks the right *structure*, not just the right tone ‚Äî wrong routing means wrong output format entirely.\n\n## üéØ What You'll Learn\n\n- Classify inputs using LLM-powered structured output (tool use)\n- Design specialized chains for structurally different content types\n- Understand why generic prompts produce mediocre results\n- Build a classifier ‚Üí specialized chain routing system\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_routing.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/02-routing/01_routing.py) | Content strategist with 3 specialized routes |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/02-routing python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/02-routing python 01_routing.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### Classification with Structured Output\n\nUses Anthropic's `tool_choice` to force structured classification output ‚Äî no parsing needed:\n\n```python\ntool_choice={\"type\": \"tool\", \"name\": \"classify_content\"}\n```\n\nThe classifier returns a `content_type` (tutorial, news, concept) and `reasoning` for transparency. By forcing a tool call, the output is always valid JSON matching the schema ‚Äî no regex or string parsing required.\n\n### Specialized Routes\n\nEach route is a mini prompt chain optimized for that content structure:\n\n- **Tutorial** (how-to): Prerequisites ‚Üí Step-by-Step ‚Üí Troubleshooting\n- **News/Announcement**: Summary of Changes ‚Üí Impact Analysis ‚Üí Call to Action\n- **Concept Explainer**: Analogy ‚Üí Architecture Description ‚Üí Pros/Cons\n\nThe key insight: a tutorial needs prerequisites before steps, a news article needs impact analysis, and a concept explainer needs analogies. A single generic prompt can't produce all three structures well.\n\n### Routing vs. Chaining\n\nRouting builds on prompt chaining (each route *is* a chain) but adds a classification step that determines which chain to execute:\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart LR\n    A[\"üó£Ô∏è Topic     \"] -->|request| B[\"‚öôÔ∏è Classify     \"]\n    B -->|tutorial| C[\"üîß Tutorial Chain     \"]\n    B -->|news| D[\"üîß News Chain     \"]\n    B -->|concept| E[\"üîß Concept Chain     \"]\n    C --> F[\"üìÑ Output     \"]\n    D --> F\n    E --> F\n```\n\nUse routing when inputs require **structurally different** processing, not just different tones or styles.\n\n### Callback Pattern\n\nLike the prompt chaining tutorial, the `ContentRouter` class emits events via a callback rather than printing directly. This keeps the class UI-agnostic ‚Äî the `main()` function decides how to render events:\n\n```python\nRouterCallback = Callable[[str, dict[str, Any]], None]\n```\n\nEvents: `classify_start`, `classify_complete`, `chain_start`, `chain_complete`.\n\n## ‚ö†Ô∏è Important Considerations\n\n- Classification accuracy is critical ‚Äî wrong route = wrong output format\n- Keep routes distinct. If two routes overlap heavily, they should be one route\n- The classifier prompt needs clear, unambiguous category definitions\n- Each route adds its own chain of LLM calls ‚Äî token cost scales with route complexity\n\n## üëâ Next Steps\n\n- [03 - Parallelization](/courses/agentic-ai-engineering/02-effective-agents/03-parallelization) ‚Äî fan-out work across independent LLM calls\n- Experiment: add a 4th route (e.g., \"Opinion/Editorial\" with a different structure)"},{"title":"Parallelization","description":"Fan-out work across multiple LLM calls simultaneously, then aggregate results","slug":"agentic-ai-engineering/02-effective-agents/03-parallelization","icon":"layers","body":"# Parallelization ‚Äî The Social Media Blast\n\nFan-out for independent work, fan-in to combine. Independent tasks run concurrently (faster), then aggregate into a single deliverable. Tasks must be truly independent ‚Äî if task B depends on task A's output, don't parallelize them.\n\n## üéØ What You'll Learn\n\n- Fan-out independent LLM calls using `ThreadPoolExecutor`\n- Aggregate parallel results into a combined output\n- Implement the voting pattern: generate candidates at different temperatures, then evaluate\n- Decouple pipeline logic from UI with event callbacks\n- Understand when tasks are truly independent vs. when they have dependencies\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_parallelization.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/03-parallelization/01_parallelization.py) | Social media promo pack + SEO title voting |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/03-parallelization python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/03-parallelization python 01_parallelization.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n### Fan-out / Fan-in\n\nA blog post (selected from `input/` or pasted custom) is sent to 3 independent writers simultaneously:\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üìÑ Blog Post     \"] -->|fan-out| B[\"üß† LinkedIn Writer     \"]\n    A -->|fan-out| C[\"üß† Twitter Writer     \"]\n    A -->|fan-out| D[\"üß† Newsletter Writer     \"]\n    B -->|result| E[\"‚öôÔ∏è Aggregator     \"]\n    C -->|result| E\n    D -->|result| E\n    E -->|combine| F[\"üìÑ Promo Pack     \"]\n```\n\nEach writer has a focused system prompt and runs as a separate thread. Results are collected as they complete and aggregated into a \"Promo Pack\" saved to `output/`.\n\n### Voting Pattern\n\nGenerate 3 SEO title candidates at different temperatures (0.3, 0.7, 1.0), then use an evaluator to pick the best one:\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üìÑ Blog Post     \"] -->|fan-out| B[\"üß† SEO Title @ 0.3     \"]\n    A -->|fan-out| C[\"üß† SEO Title @ 0.7     \"]\n    A -->|fan-out| D[\"üß† SEO Title @ 1.0     \"]\n    B -->|candidate| E[\"‚öôÔ∏è Evaluator     \"]\n    C -->|candidate| E\n    D -->|candidate| E\n    E -->|select| F[\"üè∑Ô∏è Best Title     \"]\n```\n\nLower temperatures produce safe, predictable titles. Higher temperatures produce creative, surprising ones. The evaluator picks the best from a diverse candidate pool ‚Äî more variety in, better selection out.\n\n### Thread Safety\n\nThe Anthropic Python client is thread-safe. Each `ThreadPoolExecutor` worker makes its own API call independently. Token tracking uses simple integer addition (safe for this use case).\n\n### Event Callbacks\n\nThe `ParallelContentGenerator` class emits events (`fanout_start`, `writer_complete`, `voting_start`, etc.) via a callback ‚Äî the caller decides how to render progress. This keeps the pipeline logic free of UI concerns:\n\n```python\ndef run(self, blog_post: str, on_event: GeneratorCallback | None = None) -> dict[str, str]:\n```\n\nThis is the same pattern used in [01 - Prompt Chaining](/courses/agentic-ai-engineering/02-effective-agents/01-prompt-chaining) for step progress.\n\n## ‚ö†Ô∏è Important Considerations\n\n- Tasks must be truly independent ‚Äî if task B depends on task A's output, don't parallelize\n- More concurrent calls = higher burst API usage. Watch rate limits\n- Error handling per-task: one failure shouldn't crash the whole fan-out\n- Thread count should match the number of independent tasks, not exceed it\n\n## üëâ Next Steps\n\n- [04 - Orchestrator-Workers](/courses/agentic-ai-engineering/02-effective-agents/04-orchestrator-workers) ‚Äî let the LLM dynamically decide what to parallelize\n- Experiment: add `asyncio` with `anthropic.AsyncAnthropic` for async parallelization"},{"title":"Orchestrator-Workers","description":"A central LLM dynamically decomposes tasks and delegates to parallel workers","slug":"agentic-ai-engineering/02-effective-agents/04-orchestrator-workers","icon":"network","body":"# Orchestrator-Workers ‚Äî The Deep Dive Researcher\n\nA central LLM dynamically breaks down a task, delegates subtasks to worker LLMs, and synthesizes their results. The programmer defines worker capabilities, not specific tasks.\n\n## üéØ What You'll Learn\n\n- Use an LLM as an orchestrator that dynamically plans task decomposition\n- Define worker capabilities while letting the orchestrator decide specific tasks\n- Parallelize worker execution for throughput\n- Synthesize diverse research into a coherent final output\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_orchestrator_workers.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/04-orchestrator-workers/01_orchestrator_workers.py) | Deep dive researcher with dynamic subtopic planning |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/04-orchestrator-workers python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/04-orchestrator-workers python 01_orchestrator_workers.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üó£Ô∏è Topic     \"] -->|request| B[\"üß† Orchestrator     \"]\n    B -->|\"plan (dynamic)\"| C[\"üîß Worker 1     \"]\n    B -->|\"plan (dynamic)\"| D[\"üîß Worker 2     \"]\n    B -->|\"plan (dynamic)\"| E[\"üîß Worker N     \"]\n    C -->|research| F[\"üß† Synthesizer     \"]\n    D -->|research| F\n    E -->|research| F\n    F -->|combine| G[\"üìÑ Final Article     \"]\n```\n\n### Dynamic Decomposition\n\nUnlike [03 - Parallelization](/courses/agentic-ai-engineering/02-effective-agents/03-parallelization) (where you hardcode the fan-out), the orchestrator uses an LLM to decide *what* subtopics to research based on the input:\n\n```python\ntool_choice={\"type\": \"tool\", \"name\": \"create_research_plan\"}\n```\n\n\"Compare Bun vs Node.js\" might produce: Performance, NPM Compatibility, Debugging, Deployment, Community.\n\n### Worker Pattern\n\nWorkers are generic researchers ‚Äî the orchestrator gives them specific prompts. You define the worker's *capability* (research a topic in depth), not the specific task. This is the key difference from parallelization: the LLM decides the work breakdown.\n\n### Synthesis\n\nAfter all workers complete, a synthesizer combines their independent research into a coherent article with proper flow and cross-references. This is a separate LLM call with its own system prompt ‚Äî not just concatenation.\n\n## ‚ö†Ô∏è Important Considerations\n\n- The orchestrator's plan quality determines the final output quality\n- Workers are independent ‚Äî they can't reference each other's findings\n- More subtopics = more API calls = higher cost. Consider limiting to 3-5\n\n## üëâ Next Steps\n\n- [05 - Evaluator-Optimizer](/courses/agentic-ai-engineering/02-effective-agents/05-evaluator-optimizer) ‚Äî add a quality feedback loop\n- Experiment: give workers different models (fast model for simple topics, powerful for complex)"},{"title":"Evaluator-Optimizer","description":"One LLM generates, another critiques, and the cycle repeats until quality thresholds are met","slug":"agentic-ai-engineering/02-effective-agents/05-evaluator-optimizer","icon":"refresh","body":"# Evaluator-Optimizer ‚Äî The Editor's Desk\n\nOne LLM generates a response while another evaluates it in a loop, refining until a quality threshold is met. Generator and Evaluator are different prompts with different goals.\n\n## üéØ What You'll Learn\n\n- Use an LLM as a judge to score generated content on defined dimensions\n- Define evaluation criteria and enforce them via structured tool output\n- Build an evaluate ‚Üí refine loop that converges on a quality threshold\n- Separate the generator and evaluator roles to avoid conflicting incentives\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_evaluator_optimizer.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/05-evaluator-optimizer/01_evaluator_optimizer.py) | Blog post refinement with 5-dimension evaluation loop |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/05-evaluator-optimizer python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/05-evaluator-optimizer python 01_evaluator_optimizer.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üó£Ô∏è Topic     \"] -->|request| B[\"üîß Research / Haiku + üîç     \"]\n    B -->|data| C[\"üß† Write / Haiku     \"]\n    C -->|draft| D[\"‚öôÔ∏è Evaluate / Haiku     \"]\n    D -->|\"avg < 7.0\"| E[\"üß† Refine / Sonnet     \"]\n    E -->|revised| D\n    D -->|\"avg >= 7.0\"| F[\"üìÑ Final Post     \"]\n```\n\n### Pipeline: Research ‚Üí Write ‚Üí Evaluate ‚Üí Refine\n\nThe pipeline separates web search from writing to control token costs:\n\n1. **Research** ‚Äî web search gathers current data (Haiku + `web_search` tool)\n2. **Write** ‚Äî synthesizes from research data, no tools (Haiku, text only)\n3. **Evaluate** ‚Äî 5-dimension scoring via structured output (Haiku, `tool_choice`)\n4. **Refine** ‚Äî rewrites from feedback + full draft, no tools (Sonnet)\n\nWeb search injects ~25-35k input tokens per search. By isolating it to the research phase, the write and refine steps stay lean.\n\n### Separation of Concerns\n\nThe writer, evaluator, and refiner have fundamentally different goals:\n- **Writer**: produce creative, engaging content from research data\n- **Evaluator**: critically assess quality against objective criteria\n- **Refiner**: address specific feedback while maintaining voice\n\nCombining these into one prompt creates conflicting incentives. Separating them enables targeted improvement.\n\n### Structured Evaluation\n\nScores on five dimensions (1-10):\n\n- **Clarity** ‚Äî can engineers follow without re-reading?\n- **Technical Accuracy** ‚Äî is info correct and current?\n- **Structure** ‚Äî logical flow, easy to navigate?\n- **Engagement** ‚Äî would engineers want to read this?\n- **Human Voice** ‚Äî does it sound like a real person?\n\nPlus: specific issues and actionable suggestions ‚Äî fed back to the refiner as structured feedback.\n\n### Convergence\n\nThe loop terminates when average score >= threshold (default 7.0) or after max refinements (default 2). Most improvement happens in the first refinement ‚Äî diminishing returns are real:\n\n```python\nSCORE_THRESHOLD = 7.0\nMAX_REFINEMENTS = 2\n```\n\n### Token Cost Control\n\nEach phase uses only the context it needs ‚Äî no accumulated chat history:\n\n- **Isolate expensive tools** ‚Äî web search runs once in research; all other phases are text-only\n- **Right-size models** ‚Äî Haiku handles research, writing, and evaluation; Sonnet is reserved for refinement where writing quality matters most\n- **Cap output** ‚Äî structured evaluation returns compact JSON, not prose\n\n## ‚ö†Ô∏è Important Considerations\n\n- Evaluators can be overly generous or harsh ‚Äî calibrate your threshold\n- The evaluator uses Haiku (fast, cheap) for scoring; the refiner uses Sonnet for higher writing quality\n- Diminishing returns: most improvement happens in the first refinement\n\n## üëâ Next Steps\n\n- [06 - Human-in-the-Loop](/courses/agentic-ai-engineering/02-effective-agents/06-human-in-the-loop) ‚Äî add human checkpoints to the workflow\n- Experiment: adjust `SCORE_THRESHOLD` and `MAX_REFINEMENTS` to find the quality/cost balance\n- Compare output quality with and without the research phase"},{"title":"Human in the Loop","description":"Pause agentic workflows at strategic checkpoints for human review, approval, or editing","slug":"agentic-ai-engineering/02-effective-agents/06-human-in-the-loop","icon":"user-check","body":"# Human-in-the-Loop ‚Äî The Approval Gate\n\nPause the workflow at strategic checkpoints for human review. The LLM drafts an email, a human approves or rejects with feedback, and the LLM revises ‚Äî showing where human oversight adds the most value.\n\n## üéØ What You'll Learn\n\n- Place checkpoints where errors compound most ‚Äî early in the pipeline\n- Implement three response modes: approve, reject with feedback, edit directly\n- Inject a checkpoint function into the agent class to keep logic testable\n- Cap revision loops to prevent infinite human-agent ping-pong\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_human_in_the_loop.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/06-human-in-the-loop/01_human_in_the_loop.py) | Email drafting with 2 strategic checkpoints |\n\n## üöÄ Quick Start\n\n> **Prerequisites:** Python 3.11+, API keys, and uv. See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for full setup instructions.\n\n```bash\nuv run --directory 02-effective-agents/06-human-in-the-loop python {script_name}\n\n# Example\nuv run --directory 02-effective-agents/06-human-in-the-loop python 01_human_in_the_loop.py\n```\n\nOr use the [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner) VS Code extension to run the currently open script with a single click.\n\n## üîë Key Concepts\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üó£Ô∏è Request     \"] -->|request| B[\"üß† Draft     \"]\n    B -->|draft| C[\"üë§ Checkpoint 1     \"]\n    C -->|approve| D[\"üìÑ Final Email     \"]\n    C -->|\"reject + feedback\"| E[\"üß† Revise     \"]\n    C -->|edit| D\n    E -->|revised| F[\"üë§ Checkpoint 2     \"]\n    F -->|approve| D\n    F -->|\"reject + feedback\"| E\n    F -->|edit| D\n```\n\n### Checkpoint Placement\n\nTwo checkpoints, each at a different leverage level:\n\n1. **After draft** ‚Äî high leverage. Catches wrong tone, missing points, or misunderstood intent before any revision work happens.\n2. **After revision** ‚Äî confirms the feedback was incorporated. If not, the human can provide more feedback (up to `MAX_REVISIONS`).\n\n### Three Response Modes\n\nEach checkpoint offers three options:\n\n- **(y) Approve** ‚Äî continue with the current output\n- **(n) Reject + feedback** ‚Äî agent revises based on your feedback\n- **(e) Edit** ‚Äî replace the output with your own text directly\n\nThis gives the human full control: light-touch (approve), directed (feedback), or hands-on (edit).\n\n### Injectable Checkpoint Function\n\nThe `CheckpointFn` type makes the agent testable and adaptable:\n\n```python\nCheckpointFn = Callable[[str, str, str], tuple[bool, str]]\n```\n\n- In the terminal: `human_checkpoint()` prompts via Rich UI\n- In tests: pass a lambda that auto-approves\n- In production: replace with a Slack message, webhook, or UI modal\n\n### The Leverage Principle\n\nEarly checkpoints have the highest leverage. Catching a wrong tone at checkpoint 1 saves all revision work. Catching a typo at checkpoint 2 saves nothing. Design checkpoints for maximum error prevention, not maximum coverage.\n\n## ‚ö†Ô∏è Important Considerations\n\n- Too many checkpoints = human does all the work (defeats the purpose)\n- Too few checkpoints = agent makes uncorrectable mistakes\n- In production, checkpoints are async ‚Äî Slack messages, UI approvals, webhooks ‚Äî not terminal input\n- Cap revision loops (`MAX_REVISIONS`) to prevent unbounded costs\n\n## üëâ Next Steps\n\n- [07 - Content Writer](/courses/agentic-ai-engineering/02-effective-agents/07-content-writer) ‚Äî combine all patterns into a production content creation agent\n- Experiment: add a confidence score to auto-approve high-confidence drafts\n- Try replacing `human_checkpoint` with a function that logs to a file (simulating async review)"},{"title":"Content Writer","description":"Production content writer agent combining all workflow patterns with Pydantic models and async event streaming","slug":"agentic-ai-engineering/02-effective-agents/07-content-writer","icon":"pen-tool","body":"# Content Writer ‚Äî The Full Agent\n\nA production-ready content creation pipeline that composes **all six patterns** from this module into one agent with social media parallelization, SEO title voting, Pydantic data models, and a typed async event system for clean UI/agent separation.\n\n## üéØ What You'll Learn\n\n- Compose routing, orchestration, parallelization, evaluation, and human checkpoints into a single pipeline\n- Use **Pydantic models** for validated structured output and typed event streaming\n- Use **async generators** to decouple agent logic from UI rendering\n- Apply the **fan-out** pattern (social media) and **voting** pattern (SEO titles) from tutorial 03\n- Write production-quality prompts: writing voice, anti-AI patterns, type-specific instructions\n- Balance token efficiency with output quality (dual models, focused research, selective truncation)\n\n## üì¶ Available Examples\n\n| Provider | File | Description |\n|----------|------|-------------|\n| ![Anthropic](https://raw.githubusercontent.com/agenticloops-ai/agentic-ai-engineering/main/common/badges/anthropic.svg) | [01_content_writer.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/07-content-writer/01_content_writer.py) | Entry point: async event consumer + Rich UI |\n| | [content_writer/models.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/07-content-writer/content_writer/models.py) | Pydantic data models + typed event system |\n| | [content_writer/agent.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/07-content-writer/content_writer/agent.py) | Agent class with `run_stream()` async generator |\n| | [content_writer/prompts.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/07-content-writer/content_writer/prompts.py) | Production-quality system prompts |\n| | [content_writer/tools.py](https://github.com/agenticloops-ai/agentic-ai-engineering/blob/main/02-effective-agents/07-content-writer/content_writer/tools.py) | Tool schemas for structured output + web search |\n\n## üöÄ Quick Start\n\n> **Prerequisites**: See [SETUP.md](/courses/agentic-ai-engineering/SETUP.md) for environment setup.\n\n```bash\nuv run --directory 02-effective-agents/07-content-writer python 01_content_writer.py\n```\n\n## üèóÔ∏è Architecture\n\n```\n07-content-writer/\n‚îú‚îÄ‚îÄ 01_content_writer.py             # Entry point: async event consumer + Rich UI\n‚îî‚îÄ‚îÄ content_writer/                  # Agent package\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ models.py                    # Pydantic models + 20 typed event classes\n    ‚îú‚îÄ‚îÄ agent.py                     # ContentWriterAgent ‚Äî LLM calls + async run_stream()\n    ‚îú‚îÄ‚îÄ prompts.py                   # System prompts (article + social + SEO)\n    ‚îî‚îÄ‚îÄ tools.py                     # Tool schemas (classify, plan, evaluate, SEO) + web search\n```\n\n```mermaid\n---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[\"üó£Ô∏è User Input     \"] -->|topic| B[\"‚öôÔ∏è Classify (Routing)     \"]\n    B -->|content_type| HC1[\"üë§ Human Checkpoint 1     \"]\n    HC1 --> C[\"‚öôÔ∏è Plan (Orchestrator)     \"]\n    C -->|subtopics| HC2[\"üë§ Human Checkpoint 2     \"]\n    HC2 --> D[\"üîß Research (Workers)     \"]\n    D -->|\"parallel web search\"| D\n    D -->|sections| E[\"üìù Write (Chaining)     \"]\n    E -->|draft| F[\"‚öôÔ∏è Evaluate     \"]\n    F -->|\"avg < 7.0\"| G[\"üìù Refine     \"]\n    G -->|revised| F\n    F -->|\"avg >= 7.0\"| HC3[\"üë§ Human Checkpoint 3     \"]\n    HC3 -->|approved| H[\"üì£ Social Media (Fan-out)     \"]\n    H -->|\"parallel: LinkedIn + Twitter + Newsletter\"| H\n    H --> I[\"üè∑Ô∏è SEO Title (Voting)     \"]\n    I -->|\"3 candidates ‚Üí evaluator\"| I\n    I --> J[\"üìÑ Complete Package     \"]\n```\n\n## üîë Key Concepts\n\n### Pattern Composition\n\nEach pipeline phase maps to a pattern from the module:\n\n| Phase | Pattern | Tutorial | Model |\n|-------|---------|----------|-------|\n| Classify content type | Routing | 02 | Sonnet |\n| Plan research subtopics | Orchestrator | 04 | Sonnet |\n| Research in parallel | Workers | 04 | Haiku |\n| Write with type-specific voice | Prompt Chaining | 01 | Sonnet |\n| Evaluate + refine loop | Evaluator-Optimizer | 05 | Sonnet |\n| Human checkpoints | Human-in-the-Loop | 06 | ‚Äî |\n| Social media fan-out | Parallelization (fan-out) | 03 | Haiku |\n| SEO title voting | Parallelization (voting) | 03 | Haiku + Sonnet |\n\n### Typed Event System\n\nThe agent yields Pydantic events via an async generator. The entry point uses `match/case` to render each event ‚Äî zero coupling between agent logic and UI:\n\n```python\n# Agent side ‚Äî yields typed events\nasync def run_stream(self, topic, ...) -> AsyncGenerator[AgentEvent, None]:\n    yield ClassifyStartEvent()\n    classification = await asyncio.to_thread(self.classify, topic)\n    yield ClassifyDoneEvent(classification=classification)\n    ...\n\n# Entry point ‚Äî pattern matching on event types\nasync for event in agent.run_stream(topic, ...):\n    match event:\n        case ClassifyStartEvent():\n            console.print(\"Classifying...\")\n        case ClassifyDoneEvent(classification=c):\n            console.print(f\"‚úì {c.content_type.value}: {c.topic}\")\n        case SocialWriterDoneEvent(name=n):\n            console.print(f\"‚úì {n}\")\n```\n\nEach event is a Pydantic `BaseModel` with a `Literal` stage field for type discrimination:\n\n```python\nclass ClassifyDoneEvent(BaseModel):\n    stage: Literal[\"classify_done\"] = \"classify_done\"\n    classification: ClassificationResult\n```\n\n### Pydantic Data Models\n\nAll pipeline data is validated with Pydantic ‚Äî not loose dicts:\n\n```python\nclass EvaluationResult(BaseModel):\n    clarity: int = Field(ge=1, le=10)\n    technical_accuracy: int = Field(ge=1, le=10)\n    ...\n\n    @computed_field\n    @property\n    def avg_score(self) -> float:\n        return (self.clarity + self.technical_accuracy + ...) / 5\n```\n\n### Human Checkpoints via Callback\n\nThe agent accepts an `on_human_checkpoint` callback. At each gate, it constructs a `HumanCheckpointEvent` and calls the callback synchronously (via `asyncio.to_thread`). The entry point renders the checkpoint with Rich and returns the user's response:\n\n```python\n# Agent calls this at strategic decision points\napproved, feedback = on_human_checkpoint(HumanCheckpointEvent(\n    checkpoint_id=\"classification\",\n    title=\"Classification\",\n    content=f\"Type: {content_type.value}...\",\n    question=\"Classified as 'blog'. Correct?\",\n))\n```\n\nThree checkpoints: classification (override type), research plan (adjust subtopics), final review (approve for promo pack).\n\n### Social Media Fan-Out + SEO Voting\n\nAfter the article is approved, two parallelization sub-patterns from tutorial 03:\n\n**Fan-out**: LinkedIn, Twitter, and Newsletter writers run concurrently via `ThreadPoolExecutor`:\n\n```python\nwriters = {\"linkedin\": self._write_linkedin, \"twitter\": self._write_twitter, ...}\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    futures = {executor.submit(fn, article): name for name, fn in writers.items()}\n```\n\n**Voting**: 3 SEO title candidates generated at different temperatures (0.3, 0.7, 1.0), then a structured evaluator (`tool_choice` with `pick_best_title`) selects the winner:\n\n```python\nSEO_EVALUATION_TOOLS = [{\n    \"name\": \"pick_best_title\",\n    \"input_schema\": {\n        \"properties\": {\n            \"winning_title\": {\"type\": \"string\"},\n            \"reasoning\": {\"type\": \"string\"},\n        }\n    }\n}]\n```\n\n### Dual Model Strategy\n\n- **Sonnet** for classification, planning, writing, evaluation, revision, SEO voting ‚Äî quality-critical phases\n- **Haiku** for research, social media, SEO title generation ‚Äî high-volume, cost-sensitive\n\n### Rate Limit Handling\n\nParallel phases (research, social, SEO) can exceed API rate limits. The agent uses `tenacity` for exponential backoff retry on 429 errors:\n\n```python\n@retry(\n    retry=retry_if_exception_type(RateLimitError),\n    wait=wait_exponential(multiplier=2, min=30, max=120),\n    stop=stop_after_attempt(6),\n)\ndef _call_api(self, **kwargs):\n    return self.client.messages.create(**kwargs)\n```\n\n## ‚ö†Ô∏è Important Considerations\n\n- Web search is used during **research** and **revision** ‚Äî initial writing synthesizes from research data only, but revision can search for specific facts mentioned in feedback\n- Web search uses `max_uses: 1` per research worker ‚Äî each search injects ~25-35k input tokens of page content\n- Research plan is capped at 2-3 subtopics to stay within rate limits\n- Quality-critical phases (writing, evaluation, revision) receive **full untruncated content** ‚Äî no lossy compression on the Sonnet pipeline\n- Articles are truncated to 2000 chars for social media writers and 500 chars for SEO titles ‚Äî these are Haiku cost-control measures where full context isn't needed\n- The evaluate-refine loop caps at 2 refinements to bound costs\n- Social media + SEO are gated behind the final human checkpoint ‚Äî no wasted tokens on rejected articles\n- The full pipeline makes approximately 15-22 API calls per topic\n- Rate limit errors are retried with exponential backoff (30s‚Äì120s, up to 6 attempts)\n\n## üëâ Next Steps\n\n- Read `content_writer/prompts.py` to study the prompt engineering techniques\n- Experiment with different `SCORE_THRESHOLD` and `MAX_REFINEMENTS` values\n- Add more content types (news, comparison, review) to the routing\n- Add more social platforms (Reddit, Hacker News, Dev.to) to the fan-out\n- Implement persistent memory across sessions\n- Add cost tracking with budget limits for autonomous operation"},{"title":"No Framework","description":"The raw SDK baseline ‚Äî pure client code with no abstractions","slug":"agentic-ai-engineering/04-frameworks/01-no-framework","icon":"code","body":"# No Framework\n\nThe raw SDK baseline. Pure Anthropic/OpenAI client code with no abstractions. This is your reference implementation ‚Äî everything else is measured against it.\n\n## üéØ What You'll Learn\n\n- Build a complete agent using only provider SDKs\n- Understand exactly what frameworks abstract away\n- Establish a baseline for comparing framework trade-offs\n- Make informed decisions about when abstraction helps\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Pydantic AI","description":"Type-safe agent framework built on Pydantic","slug":"agentic-ai-engineering/04-frameworks/03-pydantic-ai","icon":"shield","body":"# Pydantic AI\n\nType-safe agent framework built on Pydantic. Strong typing, dependency injection, and structured outputs as first-class citizens.\n\n## üéØ What You'll Learn\n\n- Define agents with Pydantic models for type safety\n- Use dependency injection for testable agent code\n- Get structured outputs validated at runtime\n- Leverage Python's type system for agent development\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"LangGraph","description":"Graph-based agent orchestration from LangChain","slug":"agentic-ai-engineering/04-frameworks/02-langgraph","icon":"git-branch","body":"# LangGraph\n\nGraph-based agent orchestration from LangChain. Define agents as nodes and edges with built-in state management and persistence.\n\n## üéØ What You'll Learn\n\n- Model agent workflows as directed graphs\n- Use built-in state management and checkpointing\n- Implement conditional edges for dynamic routing\n- Compare graph-based approach to linear orchestration\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Google ADK","description":"Google's Agent Development Kit with multi-agent orchestration","slug":"agentic-ai-engineering/04-frameworks/04-google-adk","icon":"cpu","body":"# Google ADK\n\nGoogle's Agent Development Kit. Multi-agent orchestration with built-in tool support and Gemini integration.\n\n## üéØ What You'll Learn\n\n- Build agents using Google's ADK architecture\n- Leverage native Gemini model integration\n- Use built-in tool definitions and execution\n- Orchestrate multi-agent workflows with ADK primitives\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"AWS Strands","description":"AWS's agent SDK with native service integration","slug":"agentic-ai-engineering/04-frameworks/05-aws-strands","icon":"cloud","body":"# AWS Strands\n\nAWS's agent SDK. Model-driven development with native AWS service integration and deployment tooling.\n\n## üéØ What You'll Learn\n\n- Build agents using AWS Strands architecture\n- Integrate with AWS services natively\n- Use model-driven development patterns\n- Deploy agents with AWS infrastructure tooling\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"CrewAI","description":"Role-based multi-agent collaboration framework","slug":"agentic-ai-engineering/04-frameworks/06-crewai","icon":"users","body":"# CrewAI\n\nRole-based multi-agent collaboration. Define agents with roles, goals, and backstories that work together on complex tasks.\n\n## üéØ What You'll Learn\n\n- Define agents with distinct roles, goals, and backstories\n- Orchestrate crews for collaborative task completion\n- Configure agent communication and delegation patterns\n- Build multi-agent workflows with clear responsibilities\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"AutoGen","description":"Microsoft's multi-agent conversation framework","slug":"agentic-ai-engineering/04-frameworks/07-autogen","icon":"message-circle","body":"# AutoGen\n\nMicrosoft's multi-agent conversation framework. Agents communicate through structured conversations with configurable interaction patterns.\n\n## üéØ What You'll Learn\n\n- Set up multi-agent conversations with AutoGen\n- Configure interaction patterns and termination conditions\n- Use human-in-the-loop conversation flows\n- Build autonomous agent teams for complex tasks\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"LlamaIndex","description":"Data-centric agent framework with strong RAG primitives","slug":"agentic-ai-engineering/04-frameworks/08-llamaindex","icon":"database","body":"# LlamaIndex\n\nData-centric agent framework. Strong RAG primitives, knowledge graph integration, and document-aware agents.\n\n## üéØ What You'll Learn\n\n- Build data-aware agents with LlamaIndex\n- Use built-in RAG and retrieval pipelines\n- Integrate knowledge graphs for complex reasoning\n- Create agents that work naturally with documents\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Semantic Kernel","description":"Microsoft's AI orchestration SDK with plugin architecture","slug":"agentic-ai-engineering/04-frameworks/09-semantic-kernel","icon":"puzzle","body":"# Semantic Kernel\n\nMicrosoft's AI orchestration SDK. Plugin architecture, planner system, and deep integration with Azure services.\n\n## üéØ What You'll Learn\n\n- Build agents using Semantic Kernel's plugin system\n- Use the planner for automatic task decomposition\n- Integrate with Azure AI services seamlessly\n- Create skills and functions for agent capabilities\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Unit Testing Agents","description":"Mock LLM responses and test agent behavior deterministically","slug":"agentic-ai-engineering/05-testing-evaluation/01-unit-testing-agents","icon":"check-square","body":"# Unit Testing Agents\n\nMock LLM responses, test tool execution deterministically, and verify agent behavior without burning API credits on every test run.\n\n## üéØ What You'll Learn\n\n- Mock LLM responses for deterministic testing\n- Test tool execution in isolation\n- Verify agent decision-making logic\n- Build fast, cheap tests that run without API calls\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Tracing & Debugging","description":"Trace every LLM call, tool invocation, and decision point","slug":"agentic-ai-engineering/05-testing-evaluation/03-tracing-debugging","icon":"search","body":"# Tracing & Debugging\n\nTrace every LLM call, tool invocation, and decision point. When your agent does something unexpected, you need to know exactly why.\n\n## üéØ What You'll Learn\n\n- Instrument agent code for comprehensive tracing\n- Debug complex multi-step agent failures\n- Visualize execution flows and decision points\n- Use observability tools during development\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Evals","description":"Build evaluation suites that measure accuracy, quality, and regression over time","slug":"agentic-ai-engineering/05-testing-evaluation/02-evals","icon":"bar-chart","body":"# Evals\n\nBuild evaluation suites that measure accuracy, quality, and regression over time. LLM-as-judge, golden datasets, and automated scoring pipelines.\n\n## üéØ What You'll Learn\n\n- Design evaluation datasets with golden examples\n- Implement LLM-as-judge scoring patterns\n- Build automated regression testing pipelines\n- Measure quality metrics that matter for your use case\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Red Teaming & Safety","description":"Adversarial testing for agents ‚Äî prompt injection, jailbreaks, and guardrails","slug":"agentic-ai-engineering/05-testing-evaluation/04-red-teaming-safety","icon":"shield-off","body":"# Red Teaming & Safety\n\nAdversarial testing for agents. Prompt injection, jailbreaks, tool misuse, and building guardrails that hold up under attack.\n\n## üéØ What You'll Learn\n\n- Test agents against prompt injection attacks\n- Identify jailbreak vulnerabilities and tool misuse\n- Build guardrails that detect and block malicious input\n- Design defense-in-depth strategies for production\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"12-Factor Agents","description":"Principles for building production-grade agents","slug":"agentic-ai-engineering/06-production/01-twelve-factor-agents","icon":"list","body":"# 12-Factor Agents\n\nPrinciples for building production-grade agents. Inspired by the 12-factor app methodology, adapted for the unique challenges of LLM-powered systems.\n\n## üéØ What You'll Learn\n\n- Apply 12-factor principles to agent architecture\n- Design agents for portability and scalability\n- Configure agents through environment, not code\n- Build production-ready agents from day one\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Cost Optimization","description":"Token budgets, caching strategies, and model routing for cost control","slug":"agentic-ai-engineering/06-production/04-cost-optimization","icon":"dollar-sign","body":"# Cost Optimization\n\nToken budgets, caching strategies, model routing, and knowing when a smaller model is the right call. Keep costs predictable as usage grows.\n\n## üéØ What You'll Learn\n\n- Set and enforce token budgets per request\n- Implement caching to reduce redundant API calls\n- Route requests to cheaper models when appropriate\n- Track and forecast costs as usage scales\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Monitoring & Observability","description":"Metrics, structured logging, and distributed tracing for production agents","slug":"agentic-ai-engineering/06-production/03-monitoring-observability","icon":"activity","body":"# Monitoring & Observability\n\nMetrics, structured logging, and distributed tracing for production agents. Know when things break before your users tell you.\n\n## üéØ What You'll Learn\n\n- Instrument agents with meaningful metrics\n- Implement structured logging for agent actions\n- Set up distributed tracing across agent calls\n- Build dashboards and alerts for production health\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Deployment Strategies","description":"Containers, serverless, and scaling patterns for agents","slug":"agentic-ai-engineering/06-production/02-deployment-strategies","icon":"upload-cloud","body":"# Deployment Strategies\n\nContainers, serverless, and scaling patterns. How to package and ship agents that handle variable load and long-running tasks.\n\n## üéØ What You'll Learn\n\n- Package agents in containers for consistent deployment\n- Choose between serverless and long-running deployments\n- Scale agents to handle variable traffic loads\n- Handle long-running tasks without timeouts\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."},{"title":"Security & Guardrails","description":"Authentication, sandboxing, and prompt injection defense","slug":"agentic-ai-engineering/06-production/05-security-guardrails","icon":"lock","body":"# Security & Guardrails\n\nAuthentication, sandboxing, prompt injection defense, and tool-use permissions. Build agents that are safe to expose to untrusted input.\n\n## üéØ What You'll Learn\n\n- Implement authentication and authorization for agent access\n- Sandbox tool execution to limit blast radius\n- Defend against prompt injection attacks\n- Design permission systems for tool-use safety\n\n> üöß **Coming soon** ‚Äî [Subscribe to our Substack](https://agenticloopsai.substack.com) or ‚≠êÔ∏è star the repo to get notified when this tutorial drops."}]