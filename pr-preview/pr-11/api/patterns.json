[{"id":"reasoning/react","number":"1.1","name":"ReAct (Reason + Act)","slug":"react","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Reasoning-Action Loop, Tool-Augmented Reasoning, Agent Loop","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Interleave chain-of-thought reasoning with tool-invoked actions and environmental observations in a tight loop, enabling the agent to ground its reasoning in real-world data.","problemContext":"You need an agent that can answer questions or complete tasks requiring external information (APIs, databases, web search) â€” not just what's in the LLM's training data.","forces":["**Reasoning depth** vs **Groundedness**: Pure chain-of-thought can hallucinate; pure tool use lacks strategic thinking","**Autonomy** vs **Cost**: More reasoning steps improve quality but consume more tokens","**Flexibility** vs **Predictability**: The agent decides its own action sequence, which is powerful but harder to debug"],"solutionDescription":"The agent operates in a loop of three phases: **Thought** (reason about what to do next), **Action** (invoke a tool or API), and **Observation** (process the result). This cycle repeats until the agent has enough information to produce a final answer. The key insight is that reasoning traces and tool results share the same context window, allowing the agent to adaptively plan based on what it learns.","diagramMermaid":"graph LR\n    Q[User Query] --> T[ğŸ§  Thought]\n    T --> A[âš¡ Action<br/>Tool Call]\n    A --> O[ğŸ‘ï¸ Observation<br/>Tool Result]\n    O --> D{Done?}\n    D -->|No| T\n    D -->|Yes| R[ğŸ“¤ Final Answer]\n\n    style Q fill:#e8f4f8,stroke:#2196F3,color:#000\n    style T fill:#fff3e0,stroke:#FF9800,color:#000\n    style A fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style O fill:#f3e5f5,stroke:#9C27B0,color:#000\n    style D fill:#fff9c4,stroke:#FFC107,color:#000\n    style R fill:#e8f4f8,stroke:#2196F3,color:#000","diagramDescription":"A user query enters the loop. The LLM produces a Thought (reasoning trace), selects an Action (tool call), receives an Observation (tool result), and decides whether to loop again or emit a Final Answer.","participants":[{"component":"Agent LLM","role":"Produces reasoning traces and selects tools"},{"component":"Tool Registry","role":"Available tools with schemas (name, description, parameters)"},{"component":"Tool Executor","role":"Runs the selected tool and returns results"},{"component":"Scratchpad","role":"Accumulates Thought-Action-Observation history in context"}],"useCases":[{"domain":"Customer Support","example":"Answer questions using knowledge base + order lookup","why":"Agent needs to reason about which tools to call based on the question"},{"domain":"Research","example":"Multi-step fact-finding with web search","why":"Each search result informs what to search next"},{"domain":"DevOps","example":"Diagnose infrastructure issues using monitoring APIs","why":"Agent reasons through symptoms, queries metrics, narrows root cause"},{"domain":"Data Analysis","example":"Answer questions by writing and executing SQL queries","why":"Agent formulates queries based on schema understanding, iterates on errors"}],"pros":["Grounds reasoning in real data, dramatically reducing hallucination","Flexible â€” agent adapts its plan based on intermediate results","Extensible â€” add new tools without changing the core loop","Well-supported across all major frameworks and LLM providers"],"cons":["Unpredictable number of iterations (and thus cost/latency)","Can get stuck in loops or choose suboptimal tool sequences","Entire reasoning trace grows the context window each iteration","Harder to debug than deterministic workflows"],"tradeoffs":[{"dimension":"Latency","impact":"Variable â€” 1-10+ LLM calls depending on task complexity"},{"dimension":"Cost","impact":"O(n) where n = number of reasoning steps Ã— context length"},{"dimension":"Reliability","impact":"Medium â€” can fail if tool returns unexpected results"},{"dimension":"Complexity","impact":"Low to implement, medium to debug"}],"whenToUse":["Tasks requiring dynamic, multi-step interaction with external systems","Problems where the sequence of operations can't be predetermined","When you need the agent to adaptively gather information before answering"],"whenNotToUse":["Tasks with a fixed, known sequence of steps â†’ use **Prompt Chaining** instead","When cost/latency must be strictly bounded â†’ use **ReWOO** or **Prompt Chaining**","Simple single-tool lookups â†’ direct function calling without a loop"],"relations":[{"relationship":"Uses","pattern":"Tool Use","patternSlug":"tool-use","description":"ReAct fundamentally depends on tool use for the Action step"},{"relationship":"Uses","pattern":"Chain-of-Thought","patternSlug":"chain-of-thought","description":"ReAct's Thought step is chain-of-thought reasoning"},{"relationship":"Simplified version of","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"ReAct is a single agent; Orchestrator-Workers distributes across multiple"},{"relationship":"Often combined with","pattern":"Guardrails","patternSlug":"guardrails","description":"Add input/output validation around the loop"},{"relationship":"Often combined with","pattern":"Sandboxed Execution","patternSlug":"sandboxed-execution","description":"Sandbox code execution tools within the ReAct loop"},{"relationship":"Alternative to","pattern":"ReWOO","patternSlug":"rewoo","description":"ReWOO front-loads all planning before acting; ReAct interleaves"},{"relationship":"Creates the need for","pattern":"Conversation Memory","patternSlug":"conversation-memory","description":"Long ReAct traces need summarization to fit context windows"},{"relationship":"Building block for","pattern":"Agentic RAG","patternSlug":"agentic-rag","description":"ReAct loop applied specifically to retrieval tasks"}],"realWorldExamples":["**Anthropic Claude**: Native tool use follows the ReAct pattern","**LangChain/LangGraph**: `create_react_agent` is the default agent constructor","**OpenAI Assistants API**: Tool use loop follows ReAct principles"],"references":["Yao et al., \"ReAct: Synergizing Reasoning and Acting in Language Models\" (2022) â€” [arXiv:2210.03629](https://arxiv.org/abs/2210.03629)","Anthropic, \"Building Effective Agents\" (2024) â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)","LangChain, \"LangGraph ReAct Agent\" â€” [docs](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/react.md","filename":"react.md"},{"id":"reasoning/rewoo","number":"1.2","name":"ReWOO (Reasoning Without Observation)","slug":"rewoo","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Plan-and-Execute, Upfront Planning","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Separate planning from execution â€” generate a complete plan with all tool calls upfront, execute them in batch, then synthesize results. This eliminates the token overhead of interleaving reasoning with observations.","problemContext":"Your agent tasks are expensive (many tool calls, large context windows) and the tool call sequence is reasonably predictable from the input. ReAct's iterative approach wastes tokens by including growing observation history in every reasoning step.","forces":["**Token efficiency** vs **Adaptability**: Batching is cheaper but can't adjust mid-execution","**Latency** vs **Accuracy**: Parallel execution is faster but misses dependencies between steps","**Security** vs **Flexibility**: Locking the plan before execution prevents prompt injection through tool outputs"],"solutionDescription":"The agent operates in three distinct phases: (1) **Planner** â€” the LLM generates a complete plan specifying all tool calls and their dependencies, (2) **Worker** â€” tools are executed (potentially in parallel), and (3) **Solver** â€” the LLM synthesizes all tool outputs into a final answer. Because the plan is generated before any tool output is seen, adversarial content in tool results cannot influence which tools get called.","diagramMermaid":"graph LR\n    Q[User Query] --> P[\"ğŸ“‹ Planner<br/>Generate full plan\"]\n    P --> W1[\"âš¡ Worker 1<br/>Tool Call A\"]\n    P --> W2[\"âš¡ Worker 2<br/>Tool Call B\"]\n    P --> W3[\"âš¡ Worker 3<br/>Tool Call C\"]\n    W1 --> S[\"ğŸ§© Solver<br/>Synthesize results\"]\n    W2 --> S\n    W3 --> S\n    S --> R[ğŸ“¤ Final Answer]\n\n    style Q fill:#e8f4f8,stroke:#2196F3,color:#000\n    style P fill:#fff3e0,stroke:#FF9800,color:#000\n    style W1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style W2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style W3 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style S fill:#f3e5f5,stroke:#9C27B0,color:#000\n    style R fill:#e8f4f8,stroke:#2196F3,color:#000","diagramDescription":"The user query goes to a Planner LLM which produces a complete execution plan. Multiple Workers execute tool calls in parallel. A Solver LLM receives all tool outputs and generates the final answer. The key difference from ReAct: no feedback loop between workers and planner.","participants":[{"component":"Planner LLM","role":"Generates complete tool-call plan from user query"},{"component":"Workers","role":"Execute tool calls, potentially in parallel"},{"component":"Solver LLM","role":"Synthesizes all results into a final answer"}],"useCases":[{"domain":"Comparison Tasks","example":"Compare multiple products, cities, services","why":"All data can be fetched independently in parallel"},{"domain":"Report Generation","example":"Gather data from multiple sources, then write report","why":"Predictable data needs, synthesis at the end"},{"domain":"Security-Sensitive","example":"Agent operating over untrusted data sources","why":"Plan is locked before exposure to potentially adversarial content"}],"pros":["Dramatically fewer tokens than ReAct (no growing observation history)","Tool calls can execute in parallel, reducing latency","More secure â€” tool outputs can't influence the plan (prevents prompt injection via tool results)","Predictable cost â€” number of LLM calls is always exactly 2 (plan + solve)"],"cons":["Can't adapt mid-execution if early results change what's needed","Planner must anticipate all needed tool calls from the query alone","Fails on tasks where later steps depend on earlier results","Wasted compute if some tool calls turn out to be unnecessary"],"tradeoffs":[{"dimension":"Latency","impact":"Lower â€” parallel execution + only 2 LLM calls"},{"dimension":"Cost","impact":"~50-70% cheaper than ReAct for multi-tool tasks"},{"dimension":"Reliability","impact":"Higher for predictable tasks; lower for exploratory ones"},{"dimension":"Complexity","impact":"Medium â€” requires plan parsing and dependency management"}],"whenToUse":["Tasks where the tool call sequence is predictable from the input","When token efficiency is a priority","Security-sensitive contexts with untrusted tool outputs","Embarrassingly parallel tool calls (comparisons, aggregations)"],"whenNotToUse":["Exploratory tasks where each step depends on previous results â†’ use **ReAct**","Tasks requiring human feedback between steps â†’ use **Human-in-the-Loop**","When the tool set is very large and selection requires context â†’ use **ReAct**"],"relations":[{"relationship":"Uses","pattern":"Tool Use","patternSlug":"tool-use","description":"Workers execute tool calls planned by the Planner"},{"relationship":"Alternative to","pattern":"ReAct","patternSlug":"react","description":"ReWOO is cheaper but less adaptive"},{"relationship":"Often combined with","pattern":"Parallelization","patternSlug":"parallelization","description":"Independent tool calls can run concurrently"},{"relationship":"Creates the need for","pattern":"Retry with Feedback","patternSlug":"retry-with-feedback","description":"When planned tools fail, need recovery strategy"}],"realWorldExamples":[],"references":["Xu et al., \"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models\" (2023) â€” [arXiv:2305.18323](https://arxiv.org/abs/2305.18323)","LangChain, \"Plan-and-Execute Agents\" â€” [docs](https://python.langchain.com/docs/how_to/agent_executor)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/rewoo.md","filename":"rewoo.md"},{"id":"reasoning/reflection","number":"1.3","name":"Reflection","slug":"reflection","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Self-Critique, Inner Monologue Review, Self-Refinement","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Have the LLM review and critique its own output, then revise â€” creating an iterative self-improvement loop that catches errors, fills gaps, and improves quality without external tools.","problemContext":"LLM outputs are often \"pretty good\" on the first attempt but contain subtle errors, omissions, or suboptimal phrasing. You want higher quality without adding tools or external validators.","forces":["**Quality** vs **Cost**: Each revision cycle adds another LLM call","**Depth** vs **Diminishing returns**: After 2-3 cycles, improvements are marginal","**Self-awareness** vs **Blind spots**: LLMs can catch some errors but systematically miss others"],"solutionDescription":"After generating an initial output, a second prompt (or the same LLM with a critic persona) evaluates the output against explicit criteria and produces structured feedback. The original output + feedback are then sent back for revision. This can loop multiple times, with optional stopping criteria (e.g., the critic rates the output above a threshold).","diagramMermaid":"graph TD\n    Q[User Query] --> G[\"âœï¸ Generator<br/>Produce initial output\"]\n    G --> C[\"ğŸ” Critic<br/>Review against criteria\"]\n    C --> D{Acceptable?}\n    D -->|No| G2[\"âœï¸ Generator<br/>Revise with feedback\"]\n    G2 --> C\n    D -->|Yes| R[ğŸ“¤ Final Output]\n\n    style Q fill:#e8f4f8,stroke:#2196F3,color:#000\n    style G fill:#fff3e0,stroke:#FF9800,color:#000\n    style C fill:#f3e5f5,stroke:#9C27B0,color:#000\n    style D fill:#fff9c4,stroke:#FFC107,color:#000\n    style G2 fill:#fff3e0,stroke:#FF9800,color:#000\n    style R fill:#e8f4f8,stroke:#2196F3,color:#000","diagramDescription":"A Generator produces initial output. A Critic reviews it against defined criteria and decides if it's acceptable. If not, feedback goes back to the Generator for revision. The loop repeats until the Critic approves or a maximum iteration limit is hit.","participants":[{"component":"Generator","role":"Produces and revises the output"},{"component":"Critic","role":"Evaluates output against defined criteria"},{"component":"Criteria","role":"Explicit rubric for what \"good\" looks like"}],"useCases":[{"domain":"Content Creation","example":"Blog posts, documentation, emails","why":"Multiple revision passes catch tone/accuracy issues"},{"domain":"Code Generation","example":"Generate code then self-review for bugs","why":"LLMs can catch many of their own coding errors"},{"domain":"Translation","example":"Translate then review for naturalness","why":"Self-critique improves fluency and idiomatic expression"}],"pros":["Simple to implement â€” no external tools required","Reliably improves output quality, especially for first-draft errors","Andrew Ng calls it \"surprisingly quick to implement\" with \"surprising performance gains\"","Criteria can be adjusted per use case"],"cons":["2-3x cost per output (each reflection cycle is an additional LLM call)","Can't catch errors outside the LLM's knowledge (hallucinations it doesn't know are wrong)","Diminishing returns after 2-3 iterations","Risk of over-editing â€” can make output worse by being too self-critical"],"tradeoffs":[{"dimension":"Latency","impact":"2-3x single generation (sequential)"},{"dimension":"Cost","impact":"2-3x base cost"},{"dimension":"Reliability","impact":"Higher â€” catches 40-60% of first-pass errors (typical)"},{"dimension":"Complexity","impact":"Very low"}],"whenToUse":["High-stakes content where quality matters more than speed","Tasks with clear, verifiable criteria","When you need better output but can't add external validators"],"whenNotToUse":["Latency-sensitive applications â†’ generate once with a better prompt","When errors require external verification (factual claims, math) â†’ use tool-based validation","Low-stakes tasks where first-pass quality is sufficient"],"relations":[{"relationship":"Simplified version of","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Reflection uses self-critique; Evaluator-Optimizer uses a separate evaluator LLM"},{"relationship":"Often combined with","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Add reflection as a final step in a chain"},{"relationship":"Alternative to","pattern":"Multi-Agent Debate","patternSlug":"multi-agent-debate","description":"Debate uses multiple agents; Reflection uses one"}],"realWorldExamples":[],"references":["Andrew Ng, \"Agentic Design Patterns Part 2: Reflection\" (2024) â€” [deeplearning.ai](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/)","Madaan et al., \"Self-Refine: Iterative Refinement with Self-Feedback\" (2023) â€” [arXiv:2303.17651](https://arxiv.org/abs/2303.17651)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/reflection.md","filename":"reflection.md"},{"id":"reasoning/chain-of-thought","number":"1.4","name":"Chain-of-Thought (CoT)","slug":"chain-of-thought","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Step-by-Step Reasoning, Scratchpad, Think-Aloud","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Prompt the LLM to decompose complex problems into intermediate reasoning steps before producing a final answer, dramatically improving accuracy on tasks requiring logic, math, or multi-step inference.","problemContext":"The LLM gives wrong answers on complex questions that require multi-step reasoning, especially math, logic puzzles, and code analysis.","forces":["**Accuracy** vs **Token cost**: More reasoning steps consume more tokens","**Transparency** vs **Latency**: Visible reasoning makes outputs debuggable but slower"],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    Q[Complex Query] --> CoT[\"ğŸ§  LLM<br/>'Think step by step...'\"]\n    CoT --> S1[\"Step 1: ...\"]\n    S1 --> S2[\"Step 2: ...\"]\n    S2 --> S3[\"Step 3: ...\"]\n    S3 --> A[ğŸ“¤ Final Answer]\n\n    style Q fill:#e8f4f8,stroke:#2196F3,color:#000\n    style CoT fill:#fff3e0,stroke:#FF9800,color:#000\n    style A fill:#e8f4f8,stroke:#2196F3,color:#000","diagramDescription":"A complex query is sent to the LLM with a \"think step by step\" instruction. The LLM produces a chain of intermediate reasoning steps, each building on the previous, culminating in a final answer.","participants":[],"useCases":[],"pros":["Dramatic accuracy improvements on reasoning tasks (often 20-40%+ on math/logic)","Makes the model's reasoning transparent and debuggable","Zero implementation cost â€” it's just a prompting technique"],"cons":["More output tokens (higher cost and latency)","Can produce verbose, unnecessary reasoning for simple tasks","The reasoning chain itself can contain errors that propagate"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Foundation for","pattern":"ReAct","patternSlug":"react","description":"ReAct adds tool use to CoT's reasoning traces"},{"relationship":"Alternative to","pattern":"Tree-of-Thoughts","patternSlug":"tree-of-thoughts","description":"ToT explores multiple CoT paths simultaneously"},{"relationship":"Often combined with","pattern":"Reflection","patternSlug":"reflection","description":"Critique the reasoning chain itself"}],"realWorldExamples":[],"references":["Wei et al., \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" (2022) â€” [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/chain-of-thought.md","filename":"chain-of-thought.md"},{"id":"reasoning/tree-of-thoughts","number":"1.5","name":"Tree-of-Thoughts (ToT)","slug":"tree-of-thoughts","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Branching Reasoning, Parallel Exploration","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Explore multiple reasoning paths simultaneously, evaluate each branch, and prune or backtrack â€” enabling the agent to consider alternatives before committing to a solution.","problemContext":"The problem has multiple valid approaches and a single Chain-of-Thought path may get stuck in a suboptimal reasoning direction. You need the agent to explore, compare, and select the best approach.","forces":["**Thoroughness** vs **Cost**: Exploring N paths costs NÃ— a single path","**Quality** vs **Latency**: Parallel exploration + evaluation is slower but finds better solutions","**Breadth** vs **Depth**: More branches explored means shallower exploration of each"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[Problem] --> B1[\"ğŸŒ¿ Branch 1<br/>Approach A\"]\n    Q --> B2[\"ğŸŒ¿ Branch 2<br/>Approach B\"]\n    Q --> B3[\"ğŸŒ¿ Branch 3<br/>Approach C\"]\n    B1 --> E[\"ğŸ” Evaluator<br/>Score each branch\"]\n    B2 --> E\n    B3 --> E\n    E --> Best[\"ğŸ† Best Branch\"]\n    Best --> D[\"Go deeper or<br/>return answer\"]\n\n    style Q fill:#e8f4f8,stroke:#2196F3,color:#000\n    style B1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style B2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style B3 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style E fill:#f3e5f5,stroke:#9C27B0,color:#000\n    style Best fill:#fff9c4,stroke:#FFC107,color:#000","diagramDescription":"A problem spawns multiple reasoning branches (different approaches). An evaluator scores each branch. The best branch is selected and either explored further or returned as the answer. Weak branches are pruned.","participants":[],"useCases":[],"pros":["Finds better solutions for complex, multi-path problems","Backtracking avoids commitment to dead-end reasoning","Naturally supports creative tasks (brainstorming, design)"],"cons":["3-10x cost of single Chain-of-Thought","Significant implementation complexity","Overkill for most practical problems"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Extends","pattern":"Chain-of-Thought","patternSlug":"chain-of-thought","description":"ToT is multiple parallel CoT paths with evaluation"},{"relationship":"Alternative to","pattern":"Parallelization ","patternSlug":"parallelization","description":"Voting runs the same prompt N times; ToT runs N different approaches"},{"relationship":"Often combined with","pattern":"Reflection","patternSlug":"reflection","description":"Use reflection as the branch evaluator"}],"realWorldExamples":[],"references":["Yao et al., \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\" (2023) â€” [arXiv:2305.10601](https://arxiv.org/abs/2305.10601)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/tree-of-thoughts.md","filename":"tree-of-thoughts.md"},{"id":"reasoning/reflexion","number":"1.6","name":"Reflexion","slug":"reflexion","category":"reasoning","categoryEmoji":"ğŸ§ ","categoryLabel":"Reasoning","alsoKnownAs":"Verbal Reinforcement Learning, Episodic Self-Improvement","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Maintain a persistent memory of past failures and the lessons learned from them, enabling the agent to improve its performance on future tasks without retraining.","problemContext":"Your agent makes the same types of mistakes repeatedly across conversations. Standard Reflection catches errors within a single generation but doesn't carry lessons forward.","forces":["**Learning** vs **Context cost**: Storing past reflections consumes context window","**Specificity** vs **Generalization**: Lessons from one failure may not apply broadly"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[Task] --> A[\"ğŸ¤– Agent<br/>Attempt task\"]\n    A --> E[\"ğŸ“Š Evaluator<br/>Grade the attempt\"]\n    E --> D{Pass?}\n    D -->|Yes| R[ğŸ“¤ Output]\n    D -->|No| Ref[\"ğŸª Self-Reflection<br/>What went wrong?\"]\n    Ref --> Mem[\"ğŸ’¾ Reflective Memory<br/>Store lesson learned\"]\n    Mem --> A\n\n    style Mem fill:#fff3e0,stroke:#FF9800,color:#000\n    style Ref fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"The agent attempts a task. An evaluator grades it. If failed, the agent produces a self-reflection (\"I should have checked X before doing Y\") which is stored in a reflective memory. On the next attempt, the agent's context includes all previous reflections, enabling it to avoid past mistakes.","participants":[],"useCases":[],"pros":["Learning persists across tasks (unlike single-pass Reflection)","No model retraining required","Reflections are human-readable and debuggable"],"cons":["Reflective memory grows unboundedly","Quality of self-reflections varies","Requires an evaluation function to detect failures"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Extends","pattern":"Reflection","patternSlug":"reflection","description":"Reflexion adds persistent memory to Reflection's single-pass self-critique"},{"relationship":"Creates the need for","pattern":"Conversation Memory","patternSlug":"conversation-memory","description":"Reflective memory needs summarization and pruning"},{"relationship":"Often combined with","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Use a separate evaluator to trigger reflection"}],"realWorldExamples":[],"references":["Shinn et al., \"Reflexion: Language Agents with Verbal Reinforcement Learning\" (2023) â€” [arXiv:2303.11366](https://arxiv.org/abs/2303.11366)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/reasoning/reflexion.md","filename":"reflexion.md"},{"id":"workflow/prompt-chaining","number":"2.1","name":"Prompt Chaining","slug":"prompt-chaining","category":"workflow","categoryEmoji":"â›“ï¸","categoryLabel":"Workflow","alsoKnownAs":"Sequential Pipeline, Multi-Step Workflow, Gate-and-Chain","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Decompose a complex task into a sequence of LLM calls, where each step's output becomes the next step's input, with optional programmatic gates between steps for validation, transformation, or routing.","problemContext":"A single LLM call can't reliably handle a complex, multi-faceted task. But the steps are known in advance and follow a predictable sequence.","forces":["**Reliability** vs **Latency**: Breaking into steps adds round-trips but improves each step's quality","**Control** vs **Flexibility**: Programmatic gates ensure quality but lock the workflow","**Simplicity** vs **Capability**: Each step is simple enough for the LLM to excel at"],"solutionDescription":"Break a complex task into N sequential steps. Each step gets a focused prompt, produces a specific output, and optionally passes through a programmatic validation gate before the next step begins. This is the foundational workflow pattern â€” Anthropic recommends it as the first pattern to reach for before considering agents.","diagramMermaid":"graph LR\n    I[Input] --> S1[\"Step 1<br/>Extract\"]\n    S1 --> G1{\"âœ… Gate 1<br/>Valid?\"}\n    G1 -->|Yes| S2[\"Step 2<br/>Transform\"]\n    G1 -->|No| E1[Error/Retry]\n    S2 --> G2{\"âœ… Gate 2<br/>Valid?\"}\n    G2 -->|Yes| S3[\"Step 3<br/>Generate\"]\n    G2 -->|No| E2[Error/Retry]\n    S3 --> O[Output]\n\n    style I fill:#e8f4f8,stroke:#2196F3,color:#000\n    style S1 fill:#fff3e0,stroke:#FF9800,color:#000\n    style S2 fill:#fff3e0,stroke:#FF9800,color:#000\n    style S3 fill:#fff3e0,stroke:#FF9800,color:#000\n    style G1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style G2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style O fill:#e8f4f8,stroke:#2196F3,color:#000","diagramDescription":"Input flows sequentially through Steps 1-3, each an LLM call with a focused task. Between each step, a programmatic Gate validates the output. Failed validation can trigger retries or error handling. Only valid outputs proceed to the next step.","participants":[{"component":"Steps (LLM calls)","role":"Each handles a focused sub-task"},{"component":"Gates (code)","role":"Validate, transform, or filter between steps"},{"component":"Pipeline Controller","role":"Manages the sequential flow and error handling"}],"useCases":[{"domain":"Content Pipeline","example":"Draft â†’ Review â†’ Polish â†’ Format","why":"Each step has clear input/output"},{"domain":"Data Processing","example":"Extract â†’ Validate â†’ Transform â†’ Load","why":"Validation gates between steps ensure data quality"},{"domain":"Customer Support","example":"Classify â†’ Retrieve context â†’ Generate response","why":"Each step is simpler than the combined task"}],"pros":["Highly reliable â€” each step is simple enough for the LLM to handle well","Easy to debug â€” inspect output at each gate","Deterministic flow â€” predictable cost and latency","Gates prevent error propagation"],"cons":["Inflexible â€” can't adapt the number or order of steps at runtime","Latency scales linearly with number of steps","Over-engineering risk â€” some tasks don't need decomposition"],"tradeoffs":[{"dimension":"Latency","impact":"O(N) where N = number of steps (sequential)"},{"dimension":"Cost","impact":"N Ã— single-call cost (but each call uses a shorter, focused prompt)"},{"dimension":"Reliability","impact":"High â€” gates catch errors before they propagate"},{"dimension":"Complexity","impact":"Very low â€” straightforward sequential code"}],"whenToUse":["The task has a clear sequence of steps known at design time","Each step's output is predictable enough to validate programmatically","You need maximum debuggability and control"],"whenNotToUse":["The sequence of steps depends on intermediate results â†’ use **ReAct** or **Routing**","A single well-crafted prompt can handle the entire task","The steps need to run in parallel â†’ use **Parallelization**"],"relations":[{"relationship":"Often combined with","pattern":"Routing","patternSlug":"routing","description":"Route to different chains based on input type"},{"relationship":"Creates the need for","pattern":"Guardrails","patternSlug":"guardrails","description":"Gates between steps are a form of guardrails"},{"relationship":"Simplified version of","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Chains are static; Orchestrator-Workers dynamically creates the plan"},{"relationship":"Often combined with","pattern":"Reflection","patternSlug":"reflection","description":"Add a reflection step at the end of the chain"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" (2024) â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/workflow/prompt-chaining.md","filename":"prompt-chaining.md"},{"id":"workflow/routing","number":"2.2","name":"Routing","slug":"routing","category":"workflow","categoryEmoji":"â›“ï¸","categoryLabel":"Workflow","alsoKnownAs":"Intent Classification, Dispatcher, Classifier-First","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Classify incoming input and direct it to a specialized handler â€” ensuring each category of request is handled by a purpose-built prompt, model, or workflow optimized for that type.","problemContext":"Your system handles diverse request types (e.g., billing questions, technical support, feature requests). A single general-purpose prompt handles none of them optimally.","forces":["**Specialization** vs **Maintenance**: Separate handlers are better but more to maintain","**Accuracy** vs **Cost**: A cheaper model can classify, then a more expensive model can handle"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    I[Input] --> R{\"ğŸ”€ Router<br/>Classify input\"}\n    R -->|Type A| HA[\"Handler A<br/>Specialized prompt\"]\n    R -->|Type B| HB[\"Handler B<br/>Specialized prompt\"]\n    R -->|Type C| HC[\"Handler C<br/>Specialized prompt\"]\n    R -->|Unknown| HF[\"Fallback<br/>General handler\"]\n    HA --> O[Output]\n    HB --> O\n    HC --> O\n    HF --> O\n\n    style R fill:#fff3e0,stroke:#FF9800,color:#000\n    style HA fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style HB fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style HC fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style HF fill:#ffebee,stroke:#f44336,color:#000","diagramDescription":"Input goes to a Router (classifier LLM or rule-based) which categorizes the request. Each category flows to a specialized Handler with an optimized prompt. Unknown inputs go to a Fallback handler. All handlers converge on the output.","participants":[],"useCases":[],"pros":["Each handler can be independently optimized (prompt, model, temperature)","Cost-effective â€” cheap model classifies, expensive model handles","Easy to add new categories without affecting existing ones","Classification accuracy is high for well-defined categories"],"cons":["Misclassification sends queries to the wrong handler","Doesn't handle ambiguous or multi-category inputs well","Adding many categories increases router complexity"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Often combined with","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Route to different chains based on classification"},{"relationship":"Alternative to","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Routing is static dispatch; Orchestrator dynamically decides"},{"relationship":"Creates the need for","pattern":"Guardrails","patternSlug":"guardrails","description":"Validate that routing decisions are correct"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” Routing pattern â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/workflow/routing.md","filename":"routing.md"},{"id":"workflow/tool-use","number":"2.3","name":"Tool Use","slug":"tool-use","category":"workflow","categoryEmoji":"â›“ï¸","categoryLabel":"Workflow","alsoKnownAs":"Function Calling, Tool Augmentation, API Integration","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Extend the LLM's capabilities by giving it access to external tools (APIs, databases, code execution) defined via structured schemas, enabling it to take actions in the real world.","problemContext":"The LLM needs to perform actions beyond text generation â€” look up data, run calculations, call APIs, execute code â€” but it can only produce text.","forces":["**Capability** vs **Security**: More tools = more powerful but larger attack surface","**Precision** vs **Flexibility**: Strict schemas prevent errors but limit expressiveness"],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    Q[Query] --> LLM[\"ğŸ¤– LLM<br/>Select tool + args\"]\n    LLM --> TC[\"ğŸ“‹ Tool Call<br/>{name, args}\"]\n    TC --> TE[\"âš¡ Tool Executor<br/>Run function\"]\n    TE --> TR[\"ğŸ“¦ Tool Result\"]\n    TR --> LLM2[\"ğŸ¤– LLM<br/>Incorporate result\"]\n    LLM2 --> O[Output]\n\n    style LLM fill:#fff3e0,stroke:#FF9800,color:#000\n    style TE fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"The LLM receives a query and selects a tool from its registry, generating a structured tool call (name + arguments). The Tool Executor runs the function and returns results. The LLM incorporates the result into its response.","participants":[],"useCases":[],"pros":["Dramatically extends LLM capabilities beyond text","Structured schemas make tool calls predictable and validateable","Andrew Ng identifies this as one of the \"more mature and reliable\" agentic patterns"],"cons":["Tool descriptions must be precise â€” bad descriptions lead to bad tool selection","Security risk if tools have side effects (writes, payments, deletions)","Each tool call adds latency and cost"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Foundation for","pattern":"ReAct","patternSlug":"react","description":"ReAct = reasoning loop + tool use"},{"relationship":"Often combined with","pattern":"Guardrails","patternSlug":"guardrails","description":"Validate tool inputs/outputs"},{"relationship":"Often combined with","pattern":"Sandboxed Execution","patternSlug":"sandboxed-execution","description":"Isolate tool execution when tools run code or have side effects"},{"relationship":"Specialized by","pattern":"Agentic RAG","patternSlug":"agentic-rag","description":"RAG specializes tool use for retrieval"},{"relationship":"Used by","pattern":"ReWOO","patternSlug":"rewoo","description":"ReWOO plans all tool calls upfront then executes in batch"},{"relationship":"Used by","pattern":"Agent-as-Tool","patternSlug":"agent-as-tool","description":"Wraps a sub-agent as a callable tool"}],"realWorldExamples":[],"references":["Andrew Ng, \"Agentic Design Patterns Part 3: Tool Use\" â€” [deeplearning.ai](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/)","Anthropic, \"Tool Use Documentation\" â€” [docs.anthropic.com](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/workflow/tool-use.md","filename":"tool-use.md"},{"id":"workflow/parallelization","number":"2.4","name":"Parallelization","slug":"parallelization","category":"workflow","categoryEmoji":"â›“ï¸","categoryLabel":"Workflow","alsoKnownAs":"Fan-out/Fan-in, Sectioning, Voting, Map-Reduce","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Execute multiple LLM calls simultaneously â€” either independent sub-tasks (**Sectioning**) or the same task multiple times (**Voting**) â€” then aggregate results for higher quality or lower latency.","problemContext":"You have a task that can be decomposed into independent parts, or you need higher confidence in the output than a single LLM call provides.","forces":["**Latency** vs **Cost**: Parallel execution is faster but uses NÃ— the compute","**Quality** vs **Simplicity**: Voting catches errors but requires aggregation logic"],"solutionDescription":"Anthropic defines two sub-patterns:","diagramMermaid":"graph TD\n    subgraph Sectioning\n        I1[Input] --> S1[\"Task A\"]\n        I1 --> S2[\"Task B\"]\n        I1 --> S3[\"Task C\"]\n        S1 --> A1[\"ğŸ”— Aggregate\"]\n        S2 --> A1\n        S3 --> A1\n    end\n\n    subgraph Voting\n        I2[Input] --> V1[\"Attempt 1\"]\n        I2 --> V2[\"Attempt 2\"]\n        I2 --> V3[\"Attempt 3\"]\n        V1 --> A2[\"ğŸ—³ï¸ Vote/Select\"]\n        V2 --> A2\n        V3 --> A2\n    end","diagramDescription":"Two sub-patterns. Sectioning splits input into different sub-tasks (A, B, C) and aggregates results. Voting runs the same task 3 times and selects the best through voting or comparison.","participants":[],"useCases":[],"pros":["Sectioning: Latency of slowest sub-task instead of sum of all","Voting: Significantly reduces error rate (3 attempts â†’ ~90% accuracy if single is ~70%)","Both are straightforward to implement with async"],"cons":["NÃ— cost compared to a single call","Sectioning requires the task to be genuinely decomposable","Voting aggregation can be non-trivial for open-ended outputs"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Often combined with","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Parallelize one step within a chain"},{"relationship":"Often combined with","pattern":"ReWOO","patternSlug":"rewoo","description":"ReWOO's independent tool calls can run in parallel"},{"relationship":"Often combined with","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Independent workers can run concurrently"},{"relationship":"Alternative to","pattern":"Tree-of-Thoughts","patternSlug":"tree-of-thoughts","description":"ToT explores different approaches; Voting runs the same approach multiple times"},{"relationship":"Creates the need for","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Need to evaluate which parallel result is best"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” Parallelization pattern â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/workflow/parallelization.md","filename":"parallelization.md"},{"id":"workflow/evaluator-optimizer","number":"2.5","name":"Evaluator-Optimizer","slug":"evaluator-optimizer","category":"workflow","categoryEmoji":"â›“ï¸","categoryLabel":"Workflow","alsoKnownAs":"Generator-Critic Pair, LLM-as-Judge, Inner Loop Refinement","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Pair a generator LLM with a separate evaluator LLM in an iterative loop â€” the generator produces output, the evaluator provides structured feedback, and the generator revises until the evaluator approves.","problemContext":"You need output quality higher than a single generation provides, and you have clear, articulable criteria for what \"good\" looks like. Unlike simple Reflection, you want the critic to be a separate, potentially different model or prompt.","forces":["**Quality** vs **Cost**: Each iteration doubles token usage","**Objectivity** vs **Agreement**: Evaluator and generator may have different quality models"],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    I[Input] --> Gen[\"âœï¸ Generator<br/>Produce output\"]\n    Gen --> Eval[\"ğŸ“Š Evaluator<br/>Score + feedback\"]\n    Eval --> D{Score â‰¥<br/>threshold?}\n    D -->|No| Gen\n    D -->|Yes| O[Output]\n\n    style Gen fill:#fff3e0,stroke:#FF9800,color:#000\n    style Eval fill:#f3e5f5,stroke:#9C27B0,color:#000\n    style D fill:#fff9c4,stroke:#FFC107,color:#000","diagramDescription":"Generator produces output. A separate Evaluator scores it against criteria and provides feedback. If below threshold, feedback loops back to Generator for revision. Continues until threshold met or max iterations reached.","participants":[],"useCases":[],"pros":["Higher quality than single-pass or self-reflection (separate evaluator is more objective)","Clear stopping criteria (score threshold)","Evaluator can be a different, cheaper model"],"cons":["2-6x cost (2 LLM calls per iteration Ã— multiple iterations)","Evaluator quality limits the ceiling","Can oscillate if evaluator feedback is contradictory"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Extends","pattern":"Reflection","patternSlug":"reflection","description":"Uses a separate evaluator instead of self-critique"},{"relationship":"Often combined with","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Apply eval-optimize as one step in a pipeline"},{"relationship":"Alternative to","pattern":"Multi-Agent Debate","patternSlug":"multi-agent-debate","description":"Debate uses peer agents; this uses an explicit judge"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” Evaluator-Optimizer pattern â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/workflow/evaluator-optimizer.md","filename":"evaluator-optimizer.md"},{"id":"orchestration/orchestrator-workers","number":"3.1","name":"Orchestrator-Workers","slug":"orchestrator-workers","category":"orchestration","categoryEmoji":"ğŸ­","categoryLabel":"Orchestration","alsoKnownAs":"Coordinator, Dynamic Decomposition, Hub-and-Spoke","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"A central orchestrator LLM dynamically breaks down tasks, delegates sub-tasks to specialized worker LLMs, and synthesizes their results â€” adapting the plan based on intermediate outcomes.","problemContext":"The task is too complex for a single agent but the sub-tasks can't be predetermined at design time. You need dynamic decomposition based on the specific input.","forces":["**Flexibility** vs **Control**: The orchestrator decides the plan, but this is harder to predict/debug","**Quality** vs **Cost**: Dedicated workers are better at sub-tasks but each is an LLM call","**Centralization** vs **Resilience**: Single orchestrator is a bottleneck and SPOF"],"solutionDescription":"A central Orchestrator LLM analyzes the task, creates a dynamic plan, delegates each sub-task to a specialized Worker LLM (which may have different system prompts, tools, or even models), collects results, and synthesizes a final output. Unlike Prompt Chaining, the number and type of workers is determined at runtime.","diagramMermaid":"graph TD\n    Q[User Task] --> O[\"ğŸ¯ Orchestrator<br/>Analyze & decompose\"]\n    O -->|\"Sub-task 1\"| W1[\"ğŸ‘· Worker 1<br/>Code Generation\"]\n    O -->|\"Sub-task 2\"| W2[\"ğŸ‘· Worker 2<br/>Research\"]\n    O -->|\"Sub-task 3\"| W3[\"ğŸ‘· Worker 3<br/>Data Analysis\"]\n    W1 --> O2[\"ğŸ¯ Orchestrator<br/>Collect & synthesize\"]\n    W2 --> O2\n    W3 --> O2\n    O2 --> R[ğŸ“¤ Final Result]\n\n    style O fill:#fff3e0,stroke:#FF9800,color:#000\n    style O2 fill:#fff3e0,stroke:#FF9800,color:#000\n    style W1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style W2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style W3 fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"User task goes to an Orchestrator that decomposes it into sub-tasks. Each sub-task is assigned to a specialized Worker. Workers return results to the Orchestrator, which synthesizes a final output. The key difference from Prompt Chaining: the Orchestrator decides the plan dynamically.","participants":[{"component":"Orchestrator LLM","role":"Decomposes tasks, assigns workers, synthesizes results"},{"component":"Worker LLMs","role":"Execute specific sub-tasks with specialized prompts/tools"},{"component":"Task Queue","role":"Manages pending and completed sub-tasks"},{"component":"Result Aggregator","role":"Collects and formats worker outputs for synthesis"}],"useCases":[{"domain":"Software Development","example":"\"Build feature X\" â€” needs research, coding, testing, docs","why":"Dynamic decomposition into specialized worker types"},{"domain":"Report Generation","example":"Complex report needing data analysis, visualization, writing","why":"Each section needs different expertise"},{"domain":"Content Creation","example":"Marketing campaign needing copy, design prompts, SEO analysis","why":"Parallel specialized workers for each content type"}],"pros":["Handles arbitrarily complex tasks by decomposition","Workers can be independently optimized (different models, prompts, tools)","Orchestrator can adapt plan based on intermediate results","Natural fit for tasks with clear sub-specializations"],"cons":["Most expensive pattern â€” multiple LLM calls for orchestration + workers + synthesis","Orchestrator is a single point of failure","Debugging is harder â€” must trace through decomposition + workers + synthesis","Over-engineering risk â€” many tasks don't need dynamic decomposition"],"tradeoffs":[{"dimension":"Latency","impact":"High â€” sequential: O(orchestrate + Î£workers + synthesize)"},{"dimension":"Cost","impact":"High â€” 3+ LLM calls minimum (plan + N workers + synthesis)"},{"dimension":"Reliability","impact":"Medium â€” depends on orchestrator's decomposition quality"},{"dimension":"Complexity","impact":"High â€” requires worker registry, result aggregation, error handling"}],"whenToUse":["Complex tasks that require multiple types of expertise","When the sub-task decomposition can't be predetermined","When different sub-tasks benefit from different models or tool sets"],"whenNotToUse":["Predictable pipelines â†’ use **Prompt Chaining**","Simple classification â†’ use **Routing**","When cost must be minimized â†’ use simpler patterns first"],"relations":[{"relationship":"More dynamic than","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Chains are static; Orchestrator plans dynamically"},{"relationship":"Alternative to","pattern":"Supervisor","patternSlug":"supervisor","description":"Orchestrator delegates; Supervisor monitors peer agents"},{"relationship":"Often combined with","pattern":"Parallelization","patternSlug":"parallelization","description":"Independent workers can run concurrently"},{"relationship":"Often combined with","pattern":"Dynamic Re-Planning","patternSlug":"dynamic-re-planning","description":"Orchestrator re-plans when workers report failures"},{"relationship":"Creates the need for","pattern":"Guardrails","patternSlug":"guardrails","description":"Worker outputs need validation before synthesis"},{"relationship":"Foundation for","pattern":"Hierarchical Teams","patternSlug":"hierarchical-teams","description":"Workers become teams with their own orchestrators"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” Orchestrator-Workers pattern â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)","Anthropic Cookbook, \"Orchestrator Workers\" â€” [github.com/anthropics/anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/orchestrator_workers.ipynb)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/orchestration/orchestrator-workers.md","filename":"orchestrator-workers.md"},{"id":"orchestration/supervisor","number":"3.2","name":"Supervisor","slug":"supervisor","category":"orchestration","categoryEmoji":"ğŸ­","categoryLabel":"Orchestration","alsoKnownAs":"Manager Agent, Central Coordinator, LangGraph Supervisor","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"A supervisor agent routes tasks to a pool of specialized agents, monitors their execution, and consolidates results â€” maintaining global awareness without micromanaging individual agent execution.","problemContext":"You have a set of specialized agents that each handle a domain. You need a central coordinator to route work, monitor progress, and consolidate outputs. Unlike Orchestrator-Workers, the specialized agents are persistent entities (not ad-hoc workers).","forces":[],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[User Query] --> S[\"ğŸ§‘â€ğŸ’¼ Supervisor<br/>Route & monitor\"]\n    S -->|\"Assign\"| A1[\"ğŸ¤– Agent: Search\"]\n    S -->|\"Assign\"| A2[\"ğŸ¤– Agent: Code\"]\n    S -->|\"Assign\"| A3[\"ğŸ¤– Agent: Data\"]\n    A1 -->|\"Report\"| S\n    A2 -->|\"Report\"| S\n    A3 -->|\"Report\"| S\n    S --> R[ğŸ“¤ Final Response]\n\n    style S fill:#fff3e0,stroke:#FF9800,color:#000\n    style A1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style A2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style A3 fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"A Supervisor agent receives queries and assigns them to specialized agents. Each agent reports back to the Supervisor, which consolidates results and decides if more work is needed or if the final response is ready.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Alternative to","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Orchestrator creates ad-hoc workers; Supervisor routes to persistent agents"},{"relationship":"Alternative to","pattern":"Swarm / Handoff","patternSlug":"swarm-handoff","description":"Supervisor centralizes control; Swarm distributes it"},{"relationship":"Often combined with","pattern":"Routing","patternSlug":"routing","description":"Supervisor essentially routes to specialized agents"},{"relationship":"Foundation for","pattern":"Hierarchical Teams","patternSlug":"hierarchical-teams","description":"Adds multiple levels of supervision"}],"realWorldExamples":[],"references":["LangGraph, \"Multi-Agent Supervisor\" â€” [docs](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/orchestration/supervisor.md","filename":"supervisor.md"},{"id":"orchestration/swarm-handoff","number":"3.3","name":"Swarm / Handoff","slug":"swarm-handoff","category":"orchestration","categoryEmoji":"ğŸ­","categoryLabel":"Orchestration","alsoKnownAs":"Agent Handoff, Peer-to-Peer Agents, Tool-Based Transfer","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Agents transfer full conversation control to peer agents via tool calls â€” enabling a decentralized mesh of specialized agents where routing decisions are made locally, not by a central coordinator.","problemContext":"You have multiple specialized agents, and the right agent for the job depends on context that emerges during the conversation. Centralized routing is too rigid or becomes a bottleneck.","forces":["**Decentralization** vs **Coherence**: Local decisions are flexible but can lose global context","**Specialization** vs **Handoff overhead**: Each transfer has a cost (context passing, potential information loss)"],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    U[User] --> A1[\"ğŸ¤– Triage Agent\"]\n    A1 -->|\"handoff_to(billing)\"| A2[\"ğŸ’° Billing Agent\"]\n    A2 -->|\"handoff_to(tech)\"| A3[\"ğŸ”§ Tech Agent\"]\n    A3 -->|\"handoff_to(triage)\"| A1\n    A1 --> U\n    A2 --> U\n    A3 --> U\n\n    style A1 fill:#fff3e0,stroke:#FF9800,color:#000\n    style A2 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style A3 fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"A Triage Agent receives the user's message. When it detects the user needs billing help, it calls `handoff_to(billing)` â€” a tool that transfers full conversation control to the Billing Agent. The Billing Agent can similarly hand off to Tech Agent or back to Triage. Each agent communicates directly with the user while active.","participants":[],"useCases":[],"pros":["No central bottleneck â€” agents make local routing decisions","Natural for customer service (triage â†’ specialist â†’ escalation)","Easy to add new agents without modifying a central router","Each agent maintains full conversation context during its turn"],"cons":["Can lose context during handoffs if not managed carefully","Debugging is harder â€” no central log of routing decisions","Risk of infinite handoff loops between agents","OpenAI-specific primitives; less portable across frameworks"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Alternative to","pattern":"Supervisor","patternSlug":"supervisor","description":"Swarm is decentralized; Supervisor is centralized"},{"relationship":"Alternative to","pattern":"Routing","patternSlug":"routing","description":"Routing is a single classification; Handoff is ongoing through conversation"},{"relationship":"Creates the need for","pattern":"Guardrails","patternSlug":"guardrails","description":"Prevent infinite handoff loops"}],"realWorldExamples":[],"references":["OpenAI, \"Swarm\" â€” [github.com/openai/swarm](https://github.com/openai/swarm)","OpenAI Agents SDK, \"Handoffs\" â€” [openai.github.io/openai-agents-python/handoffs](https://openai.github.io/openai-agents-python/handoffs/)","Google ADK, \"Agent Transfer\" â€” [google.github.io/adk-docs](https://google.github.io/adk-docs/agents/multi-agents/)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/orchestration/swarm-handoff.md","filename":"swarm-handoff.md"},{"id":"orchestration/hierarchical-teams","number":"3.4","name":"Hierarchical Teams","slug":"hierarchical-teams","category":"orchestration","categoryEmoji":"ğŸ­","categoryLabel":"Orchestration","alsoKnownAs":"Nested Agents, Sub-Graph Teams, Multi-Level Orchestration","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Organize agents into hierarchical teams where a top-level supervisor delegates to team leads, who in turn manage their own sub-agents â€” enabling complex organizations of specialized agents.","problemContext":"","forces":[],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    PM[\"ğŸ‘” Project Manager\"] --> TL1[\"ğŸ‘· Team Lead: Backend\"]\n    PM --> TL2[\"ğŸ‘· Team Lead: Frontend\"]\n    TL1 --> D1[\"ğŸ¤– Dev: API\"]\n    TL1 --> D2[\"ğŸ¤– Dev: Database\"]\n    TL2 --> D3[\"ğŸ¤– Dev: React\"]\n    TL2 --> D4[\"ğŸ¤– Dev: CSS\"]\n\n    style PM fill:#fff3e0,stroke:#FF9800,color:#000\n    style TL1 fill:#e8f5e9,stroke:#4CAF50,color:#000\n    style TL2 fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"A Project Manager delegates to Team Leads (Backend, Frontend). Each Team Lead manages their own specialized developers. The hierarchy enables compartmentalized expertise with clear reporting lines.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":["Very complex projects needing multiple areas of expertise with sub-specializations","When you want to contain complexity at each level of the hierarchy"],"whenNotToUse":["Most tasks â€” this is the most complex orchestration pattern. Start with Orchestrator-Workers or Supervisor."],"relations":[{"relationship":"Extends","pattern":"Supervisor","patternSlug":"supervisor","description":"Adds multiple levels of supervision"},{"relationship":"Extends","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Workers become teams with their own orchestrators"}],"realWorldExamples":[],"references":["LangGraph, \"Hierarchical Agent Teams\" â€” [docs](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/)","AutoGen, \"Magentic-One\" â€” [microsoft.github.io/autogen](https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/magentic-one.html)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/orchestration/hierarchical-teams.md","filename":"hierarchical-teams.md"},{"id":"orchestration/agent-as-tool","number":"3.5","name":"Agent-as-Tool","slug":"agent-as-tool","category":"orchestration","categoryEmoji":"ğŸ­","categoryLabel":"Orchestration","alsoKnownAs":"Sub-Agent, Nested Agent, Agent Tool Wrapping","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Wrap a sub-agent as a callable tool for a parent agent â€” the parent invokes the sub-agent like any other tool, receives its output, and retains full control of the conversation.","problemContext":"","forces":[],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    U[User] --> P[\"ğŸ¤– Parent Agent\"]\n    P -->|\"tool_call: research_agent\"| SA[\"ğŸ”§ Sub-Agent<br/>(wrapped as tool)\"]\n    SA -->|\"tool_result\"| P\n    P --> U\n\n    style P fill:#fff3e0,stroke:#FF9800,color:#000\n    style SA fill:#e8f5e9,stroke:#4CAF50,color:#000","diagramDescription":"The Parent Agent invokes a Sub-Agent as if it were a tool. The sub-agent runs independently, returns its result, and the parent continues its own reasoning with that result. Unlike Handoff, the parent retains control throughout.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Uses","pattern":"Tool Use","patternSlug":"tool-use","description":"Sub-agent is wrapped and invoked as a tool via standard tool-use protocol"},{"relationship":"Alternative to","pattern":"Swarm / Handoff","patternSlug":"swarm-handoff","description":"Agent-as-Tool retains control; Handoff transfers it"},{"relationship":"Often combined with","pattern":"ReAct","patternSlug":"react","description":"Parent uses ReAct loop with sub-agent as one of its tools"},{"relationship":"Building block for","pattern":"Hierarchical Teams","patternSlug":"hierarchical-teams","description":"Team leads can invoke team members as tools"}],"realWorldExamples":[],"references":["Google ADK, \"Agent-as-Tool\" â€” [google.github.io/adk-docs](https://google.github.io/adk-docs/agents/multi-agents/)","OpenAI Agents SDK, \"Agents as Tools\" â€” [openai.github.io](https://openai.github.io/openai-agents-python/)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/orchestration/agent-as-tool.md","filename":"agent-as-tool.md"},{"id":"memory/conversation-memory","number":"4.1","name":"Conversation Memory","slug":"conversation-memory","category":"memory","categoryEmoji":"ğŸ’¾","categoryLabel":"Memory & Context","alsoKnownAs":"Chat History, Context Window Management, Message Buffer","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Maintain conversation history within and across agent interactions, managing the tension between context richness and context window limits through strategies like windowing, summarization, and retrieval.","problemContext":"Agents need to remember what was said earlier in the conversation (short-term) and across sessions (long-term). But context windows are finite and expensive.","forces":["**Context richness** vs **Context window limits**: More history = better responses, but windows are finite","**Relevance** vs **Completeness**: Summarization loses details; full history wastes tokens on irrelevant messages","**Cost** vs **Quality**: Longer contexts cost more per LLM call"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    U[\"User Message\"] --> SM[\"Short-Term Memory<br/>Recent N messages\"]\n    SM --> LLM[\"ğŸ¤– LLM\"]\n\n    LTM[\"Long-Term Memory<br/>Vector DB / Summary\"] --> LLM\n\n    LLM --> R[\"Response\"]\n    R --> SM\n    R --> LTM\n\n    style SM fill:#e8f4f8,stroke:#2196F3,color:#000\n    style LTM fill:#fff3e0,stroke:#FF9800,color:#000\n    style LLM fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"User messages enter Short-Term Memory (recent messages in context window). Long-Term Memory (vector database or compressed summaries) supplements context with relevant past information. Both feed into the LLM. Responses are stored in both memory systems.","participants":[],"useCases":[],"pros":["Essential for any multi-turn interaction","Summarization dramatically reduces token costs for long conversations","Tiered approaches balance quality and cost effectively"],"cons":["Summarization loses nuanced details","Adding memory context to every call increases base cost","Stale summaries can mislead the agent"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Foundation for","pattern":"Agentic RAG","patternSlug":"agentic-rag","description":"RAG extends memory with external knowledge retrieval"},{"relationship":"Often needed by","pattern":"ReAct","patternSlug":"react","description":"Long ReAct traces need memory management"},{"relationship":"Creates the need for","pattern":"Reflexion","patternSlug":"reflexion","description":"Persistent memory enables cross-session learning"}],"realWorldExamples":[],"references":["Andrew Ng, \"Memory is a rapidly evolving area in agentic technology\" â€” [deeplearning.ai](https://www.deeplearning.ai/the-batch/)","LangGraph, \"Memory Management\" â€” [docs](https://langchain-ai.github.io/langgraph/)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/memory/conversation-memory.md","filename":"conversation-memory.md"},{"id":"memory/semantic-memory","number":"4.2","name":"Semantic Memory (Embeddings Store)","slug":"semantic-memory","category":"memory","categoryEmoji":"ğŸ’¾","categoryLabel":"Memory & Context","alsoKnownAs":"Knowledge Base, Vector Store, Embedding Memory","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Store factual knowledge as vector embeddings in a database, enabling the agent to retrieve relevant information based on semantic similarity rather than keyword matching.","problemContext":"","forces":[],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    D[\"ğŸ“„ Documents\"] --> E[\"Embed\"]\n    E --> VS[\"ğŸ—„ï¸ Vector Store\"]\n\n    Q[\"User Query\"] --> QE[\"Embed Query\"]\n    QE --> VS\n    VS -->|\"Top-K similar\"| C[\"ğŸ“‹ Context\"]\n    C --> LLM[\"ğŸ¤– LLM\"]\n    Q --> LLM\n    LLM --> R[\"Response\"]\n\n    style VS fill:#fff3e0,stroke:#FF9800,color:#000\n    style LLM fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"Documents are embedded and stored in a Vector Store. When a user query arrives, it's also embedded. The Vector Store retrieves the top-K most semantically similar documents. These become context for the LLM alongside the query.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Foundation for","pattern":"Agentic RAG","patternSlug":"agentic-rag","description":"Agentic RAG adds agent-driven retrieval strategies"},{"relationship":"Often combined with","pattern":"Conversation Memory","patternSlug":"conversation-memory","description":"Combine short-term chat memory with long-term knowledge"},{"relationship":"Building block for","pattern":"GraphRAG Memory","patternSlug":"graphrag-memory","description":"GraphRAG adds entity-relationship structure"}],"realWorldExamples":[],"references":[],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/memory/semantic-memory.md","filename":"semantic-memory.md"},{"id":"memory/graphrag-memory","number":"4.3","name":"GraphRAG Memory","slug":"graphrag-memory","category":"memory","categoryEmoji":"ğŸ’¾","categoryLabel":"Memory & Context","alsoKnownAs":"Knowledge Graph Memory, Entity-Relationship Memory","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Store entities and their relationships in a knowledge graph, enabling the agent to perform relational reasoning beyond vector similarity â€” answering questions about connections, paths, and networks.","problemContext":"","forces":[],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[\"Query about<br/>relationships\"] --> KG[\"ğŸ•¸ï¸ Knowledge Graph<br/>Entities + Relations\"]\n    KG --> T[\"Graph Traversal<br/>+ Retrieval\"]\n    T --> C[\"Context:<br/>Entities + Relationships\"]\n    C --> LLM[\"ğŸ¤– LLM\"]\n    LLM --> R[\"Response with<br/>relational reasoning\"]\n\n    style KG fill:#fff3e0,stroke:#FF9800,color:#000\n    style LLM fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"Queries about relationships (e.g., \"Who reports to Alice?\" or \"How are these projects connected?\") hit a Knowledge Graph. Graph traversal retrieves relevant entities and their relationships. This structured context enables the LLM to reason about connections.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":["Questions about relationships between entities (people, products, events)","When vector similarity alone misses important structural connections","Multi-hop reasoning (A relates to B, B relates to C, therefore A connects to C)"],"whenNotToUse":["Simple factual lookup â†’ use Semantic Memory / basic RAG","When maintaining a knowledge graph is too expensive for the use case"],"relations":[{"relationship":"Extends","pattern":"Semantic Memory","patternSlug":"semantic-memory","description":"Adds entity-relationship structure on top of vector similarity"},{"relationship":"Alternative to","pattern":"Agentic RAG","patternSlug":"agentic-rag","description":"GraphRAG uses structural graph traversal; Agentic RAG uses agent-driven iterative retrieval"},{"relationship":"Often combined with","pattern":"Conversation Memory","patternSlug":"conversation-memory","description":"Combine short-term chat memory with long-term relational knowledge"}],"realWorldExamples":[],"references":["Microsoft, \"GraphRAG: Unlocking LLM Discovery on Narrative Private Data\" (2024)","Weaviate, \"What is Agentic RAG?\" â€” [weaviate.io](https://weaviate.io/blog/what-is-agentic-rag)"],"status":"Draft","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/memory/graphrag-memory.md","filename":"graphrag-memory.md"},{"id":"memory/agentic-rag","number":"4.4","name":"Agentic RAG","slug":"agentic-rag","category":"memory","categoryEmoji":"ğŸ’¾","categoryLabel":"Memory & Context","alsoKnownAs":"Agent-Driven Retrieval, Adaptive RAG, Multi-Source RAG","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Turn retrieval into an agent-managed process â€” the agent decides what to retrieve, from which sources, evaluates retrieval quality, and iterates on its search strategy instead of using a fixed retrieval pipeline.","problemContext":"Standard RAG (embed query â†’ retrieve top-K â†’ generate) fails when the right documents require query reformulation, multi-source routing, or iterative refinement of search terms.","forces":["**Retrieval quality** vs **Simplicity**: Smarter retrieval needs more LLM calls","**Source diversity** vs **Consistency**: Multiple sources may conflict","**Depth** vs **Latency**: Iterative retrieval finds better docs but takes longer"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[\"User Query\"] --> A[\"ğŸ¤– RAG Agent\"]\n    A --> QP[\"ğŸ” Query Planning<br/>Decompose + reformulate\"]\n    QP --> S1[\"ğŸ“š Source 1:<br/>Vector DB\"]\n    QP --> S2[\"ğŸ“š Source 2:<br/>Web Search\"]\n    QP --> S3[\"ğŸ“š Source 3:<br/>SQL DB\"]\n    S1 --> E[\"ğŸ“Š Evaluate Results\"]\n    S2 --> E\n    S3 --> E\n    E --> D{Good<br/>enough?}\n    D -->|No| QP\n    D -->|Yes| G[\"Generate Answer<br/>with citations\"]\n    G --> R[\"ğŸ“¤ Response\"]\n\n    style A fill:#fff3e0,stroke:#FF9800,color:#000\n    style E fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"The RAG Agent receives a query and plans its retrieval strategy â€” decomposing complex queries, reformulating for different sources. It queries multiple sources (vector DB, web, SQL), evaluates the retrieved results, and iterates if quality is insufficient. Once satisfied, it generates a cited answer.","participants":[],"useCases":[],"pros":["Significantly better retrieval quality than static RAG pipelines","Handles complex queries that need reformulation or multi-source synthesis","Self-evaluating â€” knows when it doesn't have enough context"],"cons":["3-10x more expensive than standard RAG","Slower due to iterative retrieval","More complex to build and debug"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Extends","pattern":"Semantic Memory","patternSlug":"semantic-memory","description":"Adds agent-driven retrieval strategy on top of vector search"},{"relationship":"Specializes","pattern":"Tool Use","patternSlug":"tool-use","description":"Retrieval sources (vector DB, web, SQL) are tools the agent invokes"},{"relationship":"Uses","pattern":"ReAct","patternSlug":"react","description":"The retrieval process follows a ReAct-style loop"},{"relationship":"Often combined with","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Evaluate retrieval quality before generating"}],"realWorldExamples":[],"references":["Weaviate, \"What is Agentic RAG?\" â€” [weaviate.io/blog/what-is-agentic-rag](https://weaviate.io/blog/what-is-agentic-rag)","LangChain, \"Self-RAG and Corrective RAG\" â€” [blog](https://blog.langchain.com/agentic-rag-with-langgraph/)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/memory/agentic-rag.md","filename":"agentic-rag.md"},{"id":"safety/guardrails","number":"5.1","name":"Input/Output Guardrails","slug":"guardrails","category":"safety","categoryEmoji":"ğŸ›¡ï¸","categoryLabel":"Safety & Governance","alsoKnownAs":"Safety Rails, Validation Layer, Content Filters","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"Validate and filter agent inputs and outputs using programmatic checks and/or LLM-based classifiers to prevent harmful, off-topic, or policy-violating content from entering or leaving the system.","problemContext":"Agents deployed in production face adversarial inputs (prompt injection, jailbreaks), generate potentially harmful outputs (hallucinations, policy violations), and must comply with organizational or regulatory policies.","forces":["**Safety** vs **Latency**: Every guardrail check adds latency","**Strictness** vs **Usability**: Too many false positives frustrate users","**Coverage** vs **Cost**: LLM-based guardrails are more accurate but more expensive than rule-based ones"],"solutionDescription":"Wrap the agent's input and output with validation layers. Input guardrails classify and filter user messages before they reach the agent. Output guardrails validate agent responses before they reach the user. These can be rule-based (regex, keyword lists), classifier-based (lightweight ML models), or LLM-based (a separate LLM call to evaluate safety).","diagramMermaid":"graph LR\n    U[\"User Input\"] --> IG[\"ğŸ›¡ï¸ Input Guardrail<br/>Validate & filter\"]\n    IG -->|Pass| A[\"ğŸ¤– Agent\"]\n    IG -->|Fail| R1[\"ğŸš« Reject/Redirect\"]\n    A --> OG[\"ğŸ›¡ï¸ Output Guardrail<br/>Validate & filter\"]\n    OG -->|Pass| O[\"ğŸ“¤ Response\"]\n    OG -->|Fail| R2[\"ğŸ”„ Retry/Redact\"]\n\n    style IG fill:#ffebee,stroke:#f44336,color:#000\n    style OG fill:#ffebee,stroke:#f44336,color:#000\n    style A fill:#fff3e0,stroke:#FF9800,color:#000","diagramDescription":"User input passes through an Input Guardrail that validates and filters it. Passing inputs go to the Agent. The Agent's response passes through an Output Guardrail before reaching the user. Failed checks at either stage trigger rejection, redirection, retry, or redaction.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":["Any production deployment (input guardrails should be standard)","Applications handling sensitive domains (medical, legal, financial)","When the agent has access to tools with side effects"],"whenNotToUse":["Internal development/testing environments where speed matters more","When the application is purely informational with no risk"],"relations":[{"relationship":"Often combined with","pattern":"ReAct","patternSlug":"react","description":"Add input/output validation around the agent loop"},{"relationship":"Often combined with","pattern":"Tool Use","patternSlug":"tool-use","description":"Validate tool inputs/outputs before execution"},{"relationship":"Often combined with","pattern":"Prompt Chaining","patternSlug":"prompt-chaining","description":"Gates between steps are a form of guardrails"},{"relationship":"Often combined with","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Worker outputs need validation before synthesis"},{"relationship":"Complementary to","pattern":"Human-in-the-Loop","patternSlug":"human-in-the-loop","description":"Automated guardrails for routine checks; human for edge cases"},{"relationship":"Complementary to","pattern":"Sandboxed Execution","patternSlug":"sandboxed-execution","description":"Guardrails validate content; sandboxing isolates execution"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” guardrails recommendations â€” [anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)","OpenAI Agents SDK, \"Guardrails\" â€” [openai.github.io/openai-agents-python/guardrails](https://openai.github.io/openai-agents-python/guardrails/)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/safety/guardrails.md","filename":"guardrails.md"},{"id":"safety/human-in-the-loop","number":"5.2","name":"Human-in-the-Loop","slug":"human-in-the-loop","category":"safety","categoryEmoji":"ğŸ›¡ï¸","categoryLabel":"Safety & Governance","alsoKnownAs":"Human Oversight, Approval Gate, Escalation Path","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Require human approval, feedback, or oversight at critical decision points in the agent's workflow â€” ensuring humans maintain control over high-stakes, irreversible, or ambiguous actions.","problemContext":"The agent makes decisions that have real-world consequences (sending emails, modifying data, making purchases). Some decisions are too risky to fully automate, or the agent may encounter situations outside its training.","forces":["**Autonomy** vs **Safety**: More human oversight is safer but slower","**Throughput** vs **Quality**: Requiring approval on every action creates bottlenecks","**Trust** vs **Verification**: Even capable agents need oversight for high-stakes decisions"],"solutionDescription":"Insert human checkpoint gates at critical points in the agent's workflow. The agent pauses execution, presents its proposed action to a human reviewer, and waits for approval, rejection, or corrective feedback before continuing. The trigger for human review can be confidence-based, risk-based, or always-on for certain action types.","diagramMermaid":"","diagramDescription":"","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Often combined with","pattern":"Guardrails","patternSlug":"guardrails","description":"Automated guardrails for routine checks; human for edge cases"},{"relationship":"Often combined with","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Add human approval at the orchestrator level"},{"relationship":"Often combined with","pattern":"Sandboxed Execution","patternSlug":"sandboxed-execution","description":"Require human approval before executing sandboxed code"},{"relationship":"Escalation target for","pattern":"Retry with Feedback","patternSlug":"retry-with-feedback","description":"Human handles cases when automated retries are exhausted"},{"relationship":"Alternative to","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Human replaces the automated evaluator"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” human-in-the-loop recommendations â€” [anthropic.com](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/safety/human-in-the-loop.md","filename":"human-in-the-loop.md"},{"id":"safety/sandboxed-execution","number":"5.3","name":"Sandboxed Execution","slug":"sandboxed-execution","category":"safety","categoryEmoji":"ğŸ›¡ï¸","categoryLabel":"Safety & Governance","alsoKnownAs":"Code Sandbox, Isolated Execution, Container-Based Tools","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"Execute agent-generated code or tool calls in an isolated, resource-limited environment â€” preventing unintended side effects, resource exhaustion, or security breaches from code the agent produces.","problemContext":"","forces":[],"solutionDescription":"graph LR","diagramMermaid":"graph LR\n    A[\"ğŸ¤– Agent generates<br/>code/command\"] --> S[\"ğŸ—ï¸ Sandbox<br/>Container / VM\"]\n    S --> D{Exit code<br/>+ output}\n    D -->|Success| R[\"ğŸ“¤ Result\"]\n    D -->|Error| E[\"ğŸ”„ Error â†’ Agent\"]\n    D -->|Timeout| T[\"â° Kill + Report\"]\n\n    style S fill:#ffebee,stroke:#f44336,color:#000\n    style A fill:#fff3e0,stroke:#FF9800,color:#000","diagramDescription":"The Agent generates code or commands. These execute inside a Sandbox (container, VM, or restricted process) with resource limits (CPU, memory, time, network). Success returns results. Errors feed back to the agent. Timeouts are killed and reported.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":["Agent generates and executes code (data analysis, code assistants)","Agent interacts with filesystems, databases, or APIs with side effects","Any production deployment where agent actions could be destructive"],"whenNotToUse":["Agent only produces text (no code execution)","Tool calls are to well-defined, safe APIs with no side effects"],"relations":[{"relationship":"Often combined with","pattern":"Tool Use","patternSlug":"tool-use","description":"Sandbox the execution of agent-invoked tools that run code or have side effects"},{"relationship":"Often combined with","pattern":"Guardrails","patternSlug":"guardrails","description":"Complementary safety layers â€” guardrails validate content, sandbox isolates execution"},{"relationship":"Often combined with","pattern":"Human-in-the-Loop","patternSlug":"human-in-the-loop","description":"Require human approval before executing sandboxed code in high-stakes scenarios"},{"relationship":"Often combined with","pattern":"ReAct","patternSlug":"react","description":"Sandbox the code execution step within the ReAct loop"}],"realWorldExamples":[],"references":["OpenAI, \"Code Interpreter\" â€” sandboxed execution in production","E2B, \"Code Interpreting for AI Agents\" â€” [e2b.dev](https://e2b.dev)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/safety/sandboxed-execution.md","filename":"sandboxed-execution.md"},{"id":"resilience/retry-with-feedback","number":"6.1","name":"Retry with Feedback","slug":"retry-with-feedback","category":"resilience","categoryEmoji":"ğŸ”","categoryLabel":"Resilience & Evaluation","alsoKnownAs":"Error-Informed Retry, Contextual Retry, Corrective Loop","complexity":"â˜…â˜†â˜† Foundation","complexityLevel":1,"intent":"When an agent action fails, feed the error message back to the LLM as context for the next attempt â€” enabling the agent to learn from and correct its mistakes within a single task.","problemContext":"Tool calls fail (API errors, malformed arguments, unexpected responses). Simple retries repeat the same mistake. The agent needs to understand what went wrong and adjust its approach.","forces":["**Recovery** vs **Cost**: Each retry is another LLM call","**Learning** vs **Loops**: The agent might get stuck retrying the same approach"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    A[\"ğŸ¤– Agent Action\"] --> E{Success?}\n    E -->|Yes| R[\"ğŸ“¤ Result\"]\n    E -->|No| F[\"ğŸ“‹ Error Feedback<br/>Include error in context\"]\n    F --> A2[\"ğŸ¤– Agent Retry<br/>with error context\"]\n    A2 --> E2{Success?}\n    E2 -->|Yes| R\n    E2 -->|No, max retries| ESC[\"ğŸš¨ Escalate\"]\n\n    style F fill:#ffebee,stroke:#f44336,color:#000\n    style A fill:#fff3e0,stroke:#FF9800,color:#000\n    style A2 fill:#fff3e0,stroke:#FF9800,color:#000","diagramDescription":"Agent takes an action. If it fails, the error message is added to the agent's context as feedback. The agent retries with this additional context, enabling it to adjust its approach. After max retries, the failure is escalated (to a human, a different strategy, or an error response).","participants":[],"useCases":[],"pros":["Dramatically increases success rate on flaky operations","Agent adapts its approach based on specific error messages","Simple to implement â€” just append errors to context"],"cons":["Cost scales with retries (each is a full LLM call with growing context)","Can loop if the error isn't actionable (e.g., service is down)","Context window grows with each retry"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Often combined with","pattern":"ReAct","patternSlug":"react","description":"Retry failed tool calls within the ReAct loop"},{"relationship":"Creates the need for","pattern":"Human-in-the-Loop","patternSlug":"human-in-the-loop","description":"Escalation target when retries are exhausted"},{"relationship":"Alternative to","pattern":"Dynamic Re-Planning","patternSlug":"dynamic-re-planning","description":"Retry fixes the same approach; re-planning changes the approach"}],"realWorldExamples":[],"references":["Anthropic, \"Building Effective Agents\" â€” error handling in agentic loops â€” [anthropic.com](https://www.anthropic.com/research/building-effective-agents)"],"status":"Canonical","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/resilience/retry-with-feedback.md","filename":"retry-with-feedback.md"},{"id":"resilience/dynamic-re-planning","number":"6.2","name":"Dynamic Re-Planning","slug":"dynamic-re-planning","category":"resilience","categoryEmoji":"ğŸ”","categoryLabel":"Resilience & Evaluation","alsoKnownAs":"Adaptive Planning, Plan Repair, Stall Recovery","complexity":"â˜…â˜…â˜† Intermediate","complexityLevel":2,"intent":"When an agent's current plan is failing or stalling, generate an entirely new plan rather than retrying the same approach â€” enabling recovery from strategic (not just tactical) failures.","problemContext":"The agent's execution plan isn't working â€” not because of transient errors, but because the approach itself is wrong. Retrying won't help; the agent needs to rethink its strategy.","forces":[],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    P[\"ğŸ“‹ Current Plan\"] --> E[\"âš¡ Execute Steps\"]\n    E --> M[\"ğŸ“Š Monitor Progress\"]\n    M --> D{Stalled or<br/>failing?}\n    D -->|No| E\n    D -->|Yes| R[\"ğŸ”„ Re-Plan<br/>New strategy\"]\n    R --> P2[\"ğŸ“‹ New Plan\"]\n    P2 --> E\n\n    style R fill:#fff3e0,stroke:#FF9800,color:#000\n    style M fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"Agent executes steps from its current plan. A Monitor tracks progress. If the plan is stalling or failing strategically, it triggers a Re-Planning phase that generates a completely new plan. Execution continues with the new plan. This is distinct from retry â€” it changes the entire strategy, not just repeats the failed step.","participants":[],"useCases":[],"pros":[],"cons":[],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Extends","pattern":"Retry with Feedback","patternSlug":"retry-with-feedback","description":"Retry fixes one step; re-planning replaces the entire strategy"},{"relationship":"Used in","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Orchestrator re-plans when workers report failures"},{"relationship":"Inspired by","pattern":"Reflexion","patternSlug":"reflexion","description":"Both learn from failure, but re-planning acts within a task; Reflexion across tasks"}],"realWorldExamples":[],"references":["AutoGen, \"Magentic-One: Task Ledger + Progress Ledger\" â€” [microsoft.github.io/autogen](https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/magentic-one.html)","Lance Martin, \"Agent Design Patterns\" â€” [rlancemartin.github.io](https://rlancemartin.github.io/2026/01/09/agent_design/)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/resilience/dynamic-re-planning.md","filename":"dynamic-re-planning.md"},{"id":"resilience/multi-agent-debate","number":"6.3","name":"Multi-Agent Debate","slug":"multi-agent-debate","category":"resilience","categoryEmoji":"ğŸ”","categoryLabel":"Resilience & Evaluation","alsoKnownAs":"Adversarial Collaboration, Structured Disagreement, Dialectical Reasoning","complexity":"â˜…â˜…â˜… Advanced","complexityLevel":3,"intent":"Multiple agents argue different positions on a question, critique each other's reasoning, and converge toward a stronger conclusion through structured debate â€” catching errors and blind spots that single-agent approaches miss.","problemContext":"High-stakes decisions or analyses where a single agent's perspective is insufficient. You need diverse viewpoints and adversarial critique to surface hidden assumptions and errors.","forces":["**Quality** vs **Cost**: N agents debating is NÃ— the cost","**Convergence** vs **Diversity**: Too much debate converges prematurely; too little doesn't reach consensus"],"solutionDescription":"graph TD","diagramMermaid":"graph TD\n    Q[\"Question\"] --> A1[\"ğŸ¤– Agent 1<br/>Position A\"]\n    Q --> A2[\"ğŸ¤– Agent 2<br/>Position B\"]\n    Q --> A3[\"ğŸ¤– Agent 3<br/>Position C\"]\n    A1 --> D[\"ğŸ’¬ Debate Round<br/>Critique each other\"]\n    A2 --> D\n    A3 --> D\n    D --> D2{Consensus?}\n    D2 -->|No| D\n    D2 -->|Yes| J[\"âš–ï¸ Judge<br/>Synthesize final answer\"]\n    J --> R[\"ğŸ“¤ Answer\"]\n\n    style D fill:#fff3e0,stroke:#FF9800,color:#000\n    style J fill:#f3e5f5,stroke:#9C27B0,color:#000","diagramDescription":"Multiple agents each take a different position on a question. In debate rounds, they critique each other's arguments. If they reach consensus, a Judge agent synthesizes the final answer. If not, they debate additional rounds until convergence or a round limit.","participants":[],"useCases":[],"pros":["Catches errors and blind spots that single-agent approaches miss","Naturally surfaces trade-offs and nuances","Output reflects multiple expert viewpoints","Improves factual accuracy through adversarial checking"],"cons":["N agents Ã— M rounds = expensive","Can produce \"wishy-washy\" consensus instead of clear recommendations","May not converge on contentious topics","Significant implementation complexity"],"tradeoffs":[],"whenToUse":[],"whenNotToUse":[],"relations":[{"relationship":"Alternative to","pattern":"Reflection","patternSlug":"reflection","description":"Debate uses multiple agents; Reflection uses self-critique"},{"relationship":"Alternative to","pattern":"Evaluator-Optimizer","patternSlug":"evaluator-optimizer","description":"Debate is peer-based; Eval-Opt has an explicit judge"},{"relationship":"Often combined with","pattern":"Orchestrator-Workers","patternSlug":"orchestrator-workers","description":"Orchestrator triggers debate when high-stakes decisions are needed"}],"realWorldExamples":[],"references":["Liang et al., \"Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\" (EMNLP 2024)","Du et al., \"Improving Factuality and Reasoning in Language Models through Multiagent Debate\" (2023) â€” [arXiv:2305.14325](https://arxiv.org/abs/2305.14325)"],"status":"Established","lastUpdated":"2025-02-14","githubUrl":"https://github.com/agenticloops-ai/agentic-ai-patterns/blob/main/patterns/resilience/multi-agent-debate.md","filename":"multi-agent-debate.md"}]